{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe19873-9ec2-489b-9a9e-c85a167f5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a71c2447-e029-44c2-a5dd-f81f3135b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/6qwnpvfn0cs2whczwk_5pvqh0000gs/T/ipykernel_14638/933430717.py:5: FutureWarning: Behavior when concatenating bool-dtype and numeric-dtype arrays is deprecated; in a future version these will cast to object dtype (instead of coercing bools to numeric values). To retain the old behavior, explicitly cast bool-dtype arrays to numeric dtype.\n",
      "  train_test = pd.concat([train, test], axis=0, ignore_index=True, sort=False)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../../dataset/train.csv')\n",
    "test = pd.read_csv('../../dataset/test.csv')\n",
    "# 前処理を一度にやるためにtrainとtestをconcatする\n",
    "test['Transported'] = np.nan\n",
    "train_test = pd.concat([train, test], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "# split on `/` to cols (deck/num/side)\n",
    "def split_cabin(df):\n",
    "    cabin = df['Cabin'].str.split('/', expand=True).rename(columns={0: 'CabinDeck', 1: 'CabinNum', 2: 'CabinSide'})\n",
    "    cabin['CabinNum'] = cabin['CabinNum'].astype(float)\n",
    "    return pd.concat([df, cabin], axis=1)\n",
    "\n",
    "train_test = split_cabin(train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b13abd-68c8-479d-b158-b6a5cbbf99bd",
   "metadata": {},
   "source": [
    "### 使う特徴量を選ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd293eac-97b4-4270-acbf-208d3a1ab133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_test[['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'CabinSide', 'VIP', 'Transported']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8165aad-1f3f-47ce-9493-df726b5e5f66",
   "metadata": {},
   "source": [
    "採用する特徴量\n",
    "- HomePlanet→最頻値\n",
    "- CryoSleep→最頻値\n",
    "- Destination→最頻値\n",
    "- Age→中央値で埋める\n",
    "    - 20歳以上以下でbin化する\n",
    "- VIP→VIPなしで埋める\n",
    "- CabinSide→CabinNum==82となっているCabinSideの最頻値で埋める？→Pで埋める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5066af9-21d6-4c99-81e6-8d05acd5f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値埋め\n",
    "def fillna_cols(df):\n",
    "    df['HomePlanet'] = df['HomePlanet'].fillna(df['HomePlanet'].mode()[0])\n",
    "    df['CryoSleep'] = df['CryoSleep'].fillna(df['CryoSleep'].mode()[0])\n",
    "    df['Destination'] = df['Destination'].fillna(df['Destination'].mode()[0])\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    df['VIP'] = df['VIP'].fillna(False)\n",
    "    df['CabinSide'] = df['CabinSide'].fillna('P')\n",
    "    return df\n",
    "\n",
    "train_test = fillna_cols(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "576ed72c-5be9-4646-bc02-eb9637a04bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HomePlanet        0\n",
       "CryoSleep         0\n",
       "Destination       0\n",
       "Age               0\n",
       "CabinSide         0\n",
       "VIP               0\n",
       "Transported    4277\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388f7b7-a67f-4f47-98f2-e53129e36d1a",
   "metadata": {},
   "source": [
    "#### 成人フラグを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48932e11-ca9a-43f7-93d4-50bed0a11805",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['IsAdult'] = (train_test.Age >= 20).astype(int)\n",
    "train_test.drop('Age', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9483535-afa5-4f1c-8ba7-264aee7a3dcc",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a00295ee-a09b-41cd-a287-9dc73879dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HomePlanet, Destination, CabinSideはOne-hot化\n",
    "train_test = pd.get_dummies(train_test, columns=['HomePlanet', 'Destination', 'CabinSide'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df6d12e2-e7f0-4c2e-a9d1-7344ee92dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolをintへ\n",
    "def bool2int(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == bool:\n",
    "            df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "train_test = bool2int(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07496d12-2ce0-4d1f-83ce-4260233d7b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>VIP</th>\n",
       "      <th>Transported</th>\n",
       "      <th>IsAdult</th>\n",
       "      <th>HomePlanet_Europa</th>\n",
       "      <th>HomePlanet_Mars</th>\n",
       "      <th>Destination_PSO J318.5-22</th>\n",
       "      <th>Destination_TRAPPIST-1e</th>\n",
       "      <th>CabinSide_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12965</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12966</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12967</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12968</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12969</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12970 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CryoSleep  VIP  Transported  IsAdult  HomePlanet_Europa  \\\n",
       "0              0    0          0.0        1                  1   \n",
       "1              0    0          1.0        1                  0   \n",
       "2              0    1          0.0        1                  1   \n",
       "3              0    0          0.0        1                  1   \n",
       "4              0    0          1.0        0                  0   \n",
       "...          ...  ...          ...      ...                ...   \n",
       "12965          1    0          NaN        1                  0   \n",
       "12966          0    0          NaN        1                  0   \n",
       "12967          1    0          NaN        1                  0   \n",
       "12968          0    0          NaN        1                  1   \n",
       "12969          1    0          NaN        1                  0   \n",
       "\n",
       "       HomePlanet_Mars  Destination_PSO J318.5-22  Destination_TRAPPIST-1e  \\\n",
       "0                    0                          0                        1   \n",
       "1                    0                          0                        1   \n",
       "2                    0                          0                        1   \n",
       "3                    0                          0                        1   \n",
       "4                    0                          0                        1   \n",
       "...                ...                        ...                      ...   \n",
       "12965                0                          0                        1   \n",
       "12966                0                          0                        1   \n",
       "12967                1                          0                        0   \n",
       "12968                0                          0                        1   \n",
       "12969                0                          1                        0   \n",
       "\n",
       "       CabinSide_S  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "12965            1  \n",
       "12966            0  \n",
       "12967            0  \n",
       "12968            0  \n",
       "12969            1  \n",
       "\n",
       "[12970 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8968f15-7beb-4812-8a0b-34a01ca78706",
   "metadata": {},
   "source": [
    "### モデリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fdd3ad9-ef1f-44e8-85af-e82c1f98b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f16f4267-bcef-47bc-a153-b0b77aa53e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_feats, \n",
    "                 out_feats, \n",
    "                 hid_feats=300, \n",
    "                 lr=0.01,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_feats, hid_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hid_feats, hid_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hid_feats, out_feats),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.RAdam(self.classifier.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        self.train()\n",
    "        x, y = batch\n",
    "        self.optimizer.zero_grad()\n",
    "        pr = self.forward(x)\n",
    "        loss = self.criterion(pr, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {'loss': loss, 'pred': F.sigmoid(pr)}\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        self.eval()\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            pr = self(x)\n",
    "            loss = self.criterion(pr, y)\n",
    "        return {'loss': loss, 'pred': F.sigmoid(pr)}\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pr = self(x)\n",
    "        return F.sigmoid(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7fb96-da28-4596-b01a-9cf413b4cdc9",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12159b79-3081-420f-bcf6-c362e1888c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c287669-65a1-4e2f-ae1b-a712c2974490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=3407):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 3407\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dae376e3-e139-4686-92a5-69ddc921112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameをnp.ndarrayに変換\n",
    "train = train_test[~train_test['Transported'].isna()]\n",
    "test = train_test[train_test['Transported'].isna()]\n",
    "# inputとlabelに分離\n",
    "x_train = train.drop('Transported', axis=1).values\n",
    "y_train = train.Transported.values[:, np.newaxis]\n",
    "x_test = test.drop('Transported', axis=1).values\n",
    "\n",
    "# Tensorにする\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# Datasetにまとめる\n",
    "train_set = torch.utils.data.TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b78f65b-261d-4d0f-b136-2ec06c7794f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8693, 8]), torch.Size([4277, 8]), torch.Size([8693, 1]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f39fdd55-4807-4a25-98f5-b8f50a152580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 200\n",
    "num_hidden = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "221e97d9-6672-4c85-8a22-8faae4735542",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- cv1 ------------\n",
      "Epoch [1/200], loss: 0.7115, val_loss: 0.1806,  acc: 0.0985, val_acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamada/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/200], loss: 0.7087, val_loss: 0.1798,  acc: 0.0975, val_acc: 0.0000\n",
      "Epoch [3/200], loss: 0.7051, val_loss: 0.1789,  acc: 0.1043, val_acc: 0.0000\n",
      "Epoch [4/200], loss: 0.7004, val_loss: 0.1774,  acc: 0.1159, val_acc: 0.0339\n",
      "Epoch [5/200], loss: 0.6938, val_loss: 0.1751,  acc: 0.2087, val_acc: 0.2340\n",
      "Epoch [6/200], loss: 0.6827, val_loss: 0.1708,  acc: 0.3204, val_acc: 0.3220\n",
      "Epoch [7/200], loss: 0.6657, val_loss: 0.1654,  acc: 0.3571, val_acc: 0.3306\n",
      "Epoch [8/200], loss: 0.6471, val_loss: 0.1599,  acc: 0.3693, val_acc: 0.3399\n",
      "Epoch [9/200], loss: 0.6317, val_loss: 0.1548,  acc: 0.3664, val_acc: 0.3393\n",
      "Epoch [10/200], loss: 0.6222, val_loss: 0.1506,  acc: 0.3540, val_acc: 0.3174\n",
      "Epoch [11/200], loss: 0.6146, val_loss: 0.1478,  acc: 0.3585, val_acc: 0.3209\n",
      "Epoch [12/200], loss: 0.6098, val_loss: 0.1460,  acc: 0.3556, val_acc: 0.3122\n",
      "Epoch [13/200], loss: 0.6111, val_loss: 0.1452,  acc: 0.3582, val_acc: 0.3122\n",
      "Epoch [14/200], loss: 0.6030, val_loss: 0.1446,  acc: 0.3527, val_acc: 0.3122\n",
      "Epoch [15/200], loss: 0.6063, val_loss: 0.1444,  acc: 0.3492, val_acc: 0.3122\n",
      "Epoch [16/200], loss: 0.6052, val_loss: 0.1440,  acc: 0.3502, val_acc: 0.3145\n",
      "Epoch [17/200], loss: 0.6043, val_loss: 0.1440,  acc: 0.3539, val_acc: 0.3122\n",
      "Epoch [18/200], loss: 0.5967, val_loss: 0.1433,  acc: 0.3530, val_acc: 0.3145\n",
      "Epoch [19/200], loss: 0.6013, val_loss: 0.1430,  acc: 0.3474, val_acc: 0.3145\n",
      "Epoch [20/200], loss: 0.5986, val_loss: 0.1428,  acc: 0.3490, val_acc: 0.3145\n",
      "Epoch [21/200], loss: 0.5994, val_loss: 0.1424,  acc: 0.3542, val_acc: 0.3145\n",
      "Epoch [22/200], loss: 0.5956, val_loss: 0.1425,  acc: 0.3469, val_acc: 0.3140\n",
      "Epoch [23/200], loss: 0.5908, val_loss: 0.1419,  acc: 0.3469, val_acc: 0.3145\n",
      "Epoch [24/200], loss: 0.5927, val_loss: 0.1418,  acc: 0.3499, val_acc: 0.3140\n",
      "Epoch [25/200], loss: 0.5917, val_loss: 0.1410,  acc: 0.3471, val_acc: 0.3140\n",
      "Epoch [26/200], loss: 0.5858, val_loss: 0.1404,  acc: 0.3480, val_acc: 0.3151\n",
      "Epoch [27/200], loss: 0.5894, val_loss: 0.1406,  acc: 0.3520, val_acc: 0.3140\n",
      "Epoch [28/200], loss: 0.5897, val_loss: 0.1403,  acc: 0.3456, val_acc: 0.3145\n",
      "Epoch [29/200], loss: 0.5858, val_loss: 0.1400,  acc: 0.3466, val_acc: 0.3140\n",
      "Epoch [30/200], loss: 0.5858, val_loss: 0.1399,  acc: 0.3441, val_acc: 0.3140\n",
      "Epoch [31/200], loss: 0.5889, val_loss: 0.1405,  acc: 0.3471, val_acc: 0.3140\n",
      "Epoch [32/200], loss: 0.5884, val_loss: 0.1405,  acc: 0.3471, val_acc: 0.3140\n",
      "Epoch [33/200], loss: 0.5881, val_loss: 0.1400,  acc: 0.3428, val_acc: 0.3140\n",
      "Epoch [34/200], loss: 0.5842, val_loss: 0.1398,  acc: 0.3424, val_acc: 0.3140\n",
      "Epoch [35/200], loss: 0.5851, val_loss: 0.1400,  acc: 0.3404, val_acc: 0.3140\n",
      "Epoch [36/200], loss: 0.5831, val_loss: 0.1396,  acc: 0.3467, val_acc: 0.3140\n",
      "Epoch [37/200], loss: 0.5895, val_loss: 0.1401,  acc: 0.3408, val_acc: 0.3140\n",
      "Epoch [38/200], loss: 0.5880, val_loss: 0.1396,  acc: 0.3411, val_acc: 0.3140\n",
      "Epoch [39/200], loss: 0.5837, val_loss: 0.1394,  acc: 0.3412, val_acc: 0.3140\n",
      "Epoch [40/200], loss: 0.5806, val_loss: 0.1394,  acc: 0.3412, val_acc: 0.3140\n",
      "Epoch [41/200], loss: 0.5863, val_loss: 0.1394,  acc: 0.3361, val_acc: 0.3145\n",
      "Epoch [42/200], loss: 0.5820, val_loss: 0.1394,  acc: 0.3460, val_acc: 0.3140\n",
      "Epoch [43/200], loss: 0.5779, val_loss: 0.1392,  acc: 0.3437, val_acc: 0.3140\n",
      "Epoch [44/200], loss: 0.5837, val_loss: 0.1393,  acc: 0.3479, val_acc: 0.3140\n",
      "Epoch [45/200], loss: 0.5847, val_loss: 0.1395,  acc: 0.3391, val_acc: 0.3140\n",
      "Epoch [46/200], loss: 0.5809, val_loss: 0.1391,  acc: 0.3438, val_acc: 0.3048\n",
      "Epoch [47/200], loss: 0.5817, val_loss: 0.1395,  acc: 0.3418, val_acc: 0.2984\n",
      "Epoch [48/200], loss: 0.5782, val_loss: 0.1391,  acc: 0.3461, val_acc: 0.3140\n",
      "Epoch [49/200], loss: 0.5887, val_loss: 0.1395,  acc: 0.3433, val_acc: 0.3140\n",
      "Epoch [50/200], loss: 0.5829, val_loss: 0.1392,  acc: 0.3437, val_acc: 0.3140\n",
      "Epoch [51/200], loss: 0.5778, val_loss: 0.1394,  acc: 0.3487, val_acc: 0.2892\n",
      "Epoch [52/200], loss: 0.5841, val_loss: 0.1392,  acc: 0.3415, val_acc: 0.3140\n",
      "Epoch [53/200], loss: 0.5863, val_loss: 0.1395,  acc: 0.3414, val_acc: 0.3140\n",
      "Epoch [54/200], loss: 0.5794, val_loss: 0.1392,  acc: 0.3458, val_acc: 0.3048\n",
      "Epoch [55/200], loss: 0.5797, val_loss: 0.1390,  acc: 0.3415, val_acc: 0.3140\n",
      "Epoch [56/200], loss: 0.5853, val_loss: 0.1393,  acc: 0.3364, val_acc: 0.3048\n",
      "Epoch [57/200], loss: 0.5811, val_loss: 0.1391,  acc: 0.3391, val_acc: 0.3048\n",
      "Epoch [58/200], loss: 0.5789, val_loss: 0.1392,  acc: 0.3471, val_acc: 0.3048\n",
      "Epoch [59/200], loss: 0.5832, val_loss: 0.1396,  acc: 0.3425, val_acc: 0.3048\n",
      "Epoch [60/200], loss: 0.5742, val_loss: 0.1390,  acc: 0.3401, val_acc: 0.3048\n",
      "Epoch [61/200], loss: 0.5835, val_loss: 0.1395,  acc: 0.3371, val_acc: 0.3048\n",
      "Epoch [62/200], loss: 0.5834, val_loss: 0.1389,  acc: 0.3382, val_acc: 0.3048\n",
      "Epoch [63/200], loss: 0.5799, val_loss: 0.1388,  acc: 0.3435, val_acc: 0.3140\n",
      "Epoch [64/200], loss: 0.5882, val_loss: 0.1395,  acc: 0.3424, val_acc: 0.3048\n",
      "Epoch [65/200], loss: 0.5831, val_loss: 0.1392,  acc: 0.3389, val_acc: 0.3048\n",
      "Epoch [66/200], loss: 0.5802, val_loss: 0.1387,  acc: 0.3410, val_acc: 0.3048\n",
      "Epoch [67/200], loss: 0.5824, val_loss: 0.1393,  acc: 0.3405, val_acc: 0.3048\n",
      "Epoch [68/200], loss: 0.5795, val_loss: 0.1393,  acc: 0.3411, val_acc: 0.3048\n",
      "Epoch [69/200], loss: 0.5821, val_loss: 0.1393,  acc: 0.3353, val_acc: 0.3048\n",
      "Epoch [70/200], loss: 0.5832, val_loss: 0.1393,  acc: 0.3427, val_acc: 0.3048\n",
      "Epoch [71/200], loss: 0.5839, val_loss: 0.1389,  acc: 0.3358, val_acc: 0.3048\n",
      "Epoch [72/200], loss: 0.5796, val_loss: 0.1391,  acc: 0.3447, val_acc: 0.3048\n",
      "Epoch [73/200], loss: 0.5797, val_loss: 0.1389,  acc: 0.3397, val_acc: 0.3048\n",
      "Epoch [74/200], loss: 0.5796, val_loss: 0.1392,  acc: 0.3397, val_acc: 0.3048\n",
      "Epoch [75/200], loss: 0.5795, val_loss: 0.1390,  acc: 0.3422, val_acc: 0.3048\n",
      "Epoch [76/200], loss: 0.5794, val_loss: 0.1390,  acc: 0.3375, val_acc: 0.3048\n",
      "Epoch [77/200], loss: 0.5764, val_loss: 0.1390,  acc: 0.3407, val_acc: 0.3048\n",
      "Epoch [78/200], loss: 0.5754, val_loss: 0.1387,  acc: 0.3411, val_acc: 0.3048\n",
      "Epoch [79/200], loss: 0.5803, val_loss: 0.1390,  acc: 0.3457, val_acc: 0.3048\n",
      "Epoch [80/200], loss: 0.5786, val_loss: 0.1388,  acc: 0.3440, val_acc: 0.3048\n",
      "Epoch [81/200], loss: 0.5809, val_loss: 0.1388,  acc: 0.3411, val_acc: 0.3048\n",
      "Epoch [82/200], loss: 0.5817, val_loss: 0.1389,  acc: 0.3384, val_acc: 0.3048\n",
      "Epoch [83/200], loss: 0.5776, val_loss: 0.1387,  acc: 0.3443, val_acc: 0.3076\n",
      "Epoch [84/200], loss: 0.5834, val_loss: 0.1390,  acc: 0.3453, val_acc: 0.3082\n",
      "Epoch [85/200], loss: 0.5829, val_loss: 0.1393,  acc: 0.3387, val_acc: 0.3053\n",
      "Epoch [86/200], loss: 0.5834, val_loss: 0.1391,  acc: 0.3382, val_acc: 0.3053\n",
      "Epoch [87/200], loss: 0.5838, val_loss: 0.1389,  acc: 0.3438, val_acc: 0.3053\n",
      "Epoch [88/200], loss: 0.5836, val_loss: 0.1388,  acc: 0.3411, val_acc: 0.3048\n",
      "Epoch [89/200], loss: 0.5847, val_loss: 0.1393,  acc: 0.3431, val_acc: 0.3393\n",
      "Epoch [90/200], loss: 0.5802, val_loss: 0.1390,  acc: 0.3450, val_acc: 0.3053\n",
      "Epoch [91/200], loss: 0.5799, val_loss: 0.1393,  acc: 0.3438, val_acc: 0.3053\n",
      "Epoch [92/200], loss: 0.5790, val_loss: 0.1389,  acc: 0.3411, val_acc: 0.3053\n",
      "Epoch [93/200], loss: 0.5825, val_loss: 0.1390,  acc: 0.3502, val_acc: 0.3053\n",
      "Epoch [94/200], loss: 0.5796, val_loss: 0.1392,  acc: 0.3410, val_acc: 0.3053\n",
      "Epoch [95/200], loss: 0.5815, val_loss: 0.1388,  acc: 0.3399, val_acc: 0.3266\n",
      "Epoch [96/200], loss: 0.5831, val_loss: 0.1391,  acc: 0.3479, val_acc: 0.3111\n",
      "Epoch [97/200], loss: 0.5825, val_loss: 0.1391,  acc: 0.3431, val_acc: 0.3053\n",
      "Epoch [98/200], loss: 0.5794, val_loss: 0.1390,  acc: 0.3418, val_acc: 0.3053\n",
      "Epoch [99/200], loss: 0.5790, val_loss: 0.1391,  acc: 0.3454, val_acc: 0.3053\n",
      "Epoch [100/200], loss: 0.5809, val_loss: 0.1392,  acc: 0.3408, val_acc: 0.3111\n",
      "Epoch [101/200], loss: 0.5790, val_loss: 0.1391,  acc: 0.3421, val_acc: 0.3053\n",
      "Epoch [102/200], loss: 0.5816, val_loss: 0.1389,  acc: 0.3407, val_acc: 0.3053\n",
      "Epoch [103/200], loss: 0.5830, val_loss: 0.1393,  acc: 0.3412, val_acc: 0.3111\n",
      "Epoch [104/200], loss: 0.5782, val_loss: 0.1388,  acc: 0.3447, val_acc: 0.3053\n",
      "Epoch [105/200], loss: 0.5832, val_loss: 0.1392,  acc: 0.3437, val_acc: 0.3053\n",
      "Epoch [106/200], loss: 0.5820, val_loss: 0.1387,  acc: 0.3430, val_acc: 0.3053\n",
      "Epoch [107/200], loss: 0.5834, val_loss: 0.1395,  acc: 0.3391, val_acc: 0.3053\n",
      "Epoch [108/200], loss: 0.5828, val_loss: 0.1394,  acc: 0.3450, val_acc: 0.3053\n",
      "Epoch [109/200], loss: 0.5767, val_loss: 0.1387,  acc: 0.3433, val_acc: 0.3053\n",
      "Epoch [110/200], loss: 0.5796, val_loss: 0.1390,  acc: 0.3425, val_acc: 0.3053\n",
      "Epoch [111/200], loss: 0.5816, val_loss: 0.1387,  acc: 0.3343, val_acc: 0.3145\n",
      "Epoch [112/200], loss: 0.5798, val_loss: 0.1392,  acc: 0.3506, val_acc: 0.3053\n",
      "Epoch [113/200], loss: 0.5779, val_loss: 0.1391,  acc: 0.3440, val_acc: 0.3053\n",
      "Epoch [114/200], loss: 0.5802, val_loss: 0.1387,  acc: 0.3392, val_acc: 0.3053\n",
      "Epoch [115/200], loss: 0.5835, val_loss: 0.1392,  acc: 0.3391, val_acc: 0.3059\n",
      "Epoch [116/200], loss: 0.5810, val_loss: 0.1390,  acc: 0.3407, val_acc: 0.3145\n",
      "Epoch [117/200], loss: 0.5847, val_loss: 0.1396,  acc: 0.3437, val_acc: 0.3053\n",
      "Epoch [118/200], loss: 0.5786, val_loss: 0.1387,  acc: 0.3375, val_acc: 0.3053\n",
      "Epoch [119/200], loss: 0.5782, val_loss: 0.1386,  acc: 0.3417, val_acc: 0.3053\n",
      "Epoch [120/200], loss: 0.5799, val_loss: 0.1389,  acc: 0.3372, val_acc: 0.3111\n",
      "Epoch [121/200], loss: 0.5750, val_loss: 0.1389,  acc: 0.3443, val_acc: 0.3053\n",
      "Epoch [122/200], loss: 0.5818, val_loss: 0.1389,  acc: 0.3422, val_acc: 0.3053\n",
      "Epoch [123/200], loss: 0.5784, val_loss: 0.1386,  acc: 0.3412, val_acc: 0.3053\n",
      "Epoch [124/200], loss: 0.5834, val_loss: 0.1394,  acc: 0.3450, val_acc: 0.3053\n",
      "Epoch [125/200], loss: 0.5779, val_loss: 0.1391,  acc: 0.3486, val_acc: 0.3053\n",
      "Epoch [126/200], loss: 0.5793, val_loss: 0.1389,  acc: 0.3407, val_acc: 0.3053\n",
      "Epoch [127/200], loss: 0.5788, val_loss: 0.1390,  acc: 0.3381, val_acc: 0.3053\n",
      "Epoch [128/200], loss: 0.5828, val_loss: 0.1391,  acc: 0.3371, val_acc: 0.3053\n",
      "Epoch [129/200], loss: 0.5813, val_loss: 0.1391,  acc: 0.3401, val_acc: 0.3053\n",
      "Epoch [130/200], loss: 0.5829, val_loss: 0.1389,  acc: 0.3378, val_acc: 0.3111\n",
      "Epoch [131/200], loss: 0.5788, val_loss: 0.1388,  acc: 0.3404, val_acc: 0.3117\n",
      "Epoch [132/200], loss: 0.5801, val_loss: 0.1389,  acc: 0.3480, val_acc: 0.3053\n",
      "Epoch [133/200], loss: 0.5813, val_loss: 0.1392,  acc: 0.3448, val_acc: 0.3053\n",
      "Epoch [134/200], loss: 0.5779, val_loss: 0.1390,  acc: 0.3476, val_acc: 0.3053\n",
      "Epoch [135/200], loss: 0.5757, val_loss: 0.1390,  acc: 0.3407, val_acc: 0.3053\n",
      "Epoch [136/200], loss: 0.5764, val_loss: 0.1387,  acc: 0.3448, val_acc: 0.3111\n",
      "Epoch [137/200], loss: 0.5858, val_loss: 0.1391,  acc: 0.3414, val_acc: 0.3111\n",
      "Epoch [138/200], loss: 0.5728, val_loss: 0.1386,  acc: 0.3487, val_acc: 0.3111\n",
      "Epoch [139/200], loss: 0.5806, val_loss: 0.1390,  acc: 0.3480, val_acc: 0.3053\n",
      "Epoch [140/200], loss: 0.5769, val_loss: 0.1389,  acc: 0.3421, val_acc: 0.3053\n",
      "Epoch [141/200], loss: 0.5788, val_loss: 0.1389,  acc: 0.3467, val_acc: 0.3053\n",
      "Epoch [142/200], loss: 0.5846, val_loss: 0.1390,  acc: 0.3405, val_acc: 0.3053\n",
      "Epoch [143/200], loss: 0.5810, val_loss: 0.1390,  acc: 0.3362, val_acc: 0.3111\n",
      "Epoch [144/200], loss: 0.5802, val_loss: 0.1385,  acc: 0.3421, val_acc: 0.3145\n",
      "Epoch [145/200], loss: 0.5838, val_loss: 0.1390,  acc: 0.3399, val_acc: 0.3111\n",
      "Epoch [146/200], loss: 0.5786, val_loss: 0.1391,  acc: 0.3437, val_acc: 0.3053\n",
      "Epoch [147/200], loss: 0.5843, val_loss: 0.1391,  acc: 0.3371, val_acc: 0.3145\n",
      "Epoch [148/200], loss: 0.5799, val_loss: 0.1388,  acc: 0.3424, val_acc: 0.3053\n",
      "Epoch [149/200], loss: 0.5800, val_loss: 0.1389,  acc: 0.3397, val_acc: 0.3111\n",
      "Epoch [150/200], loss: 0.5812, val_loss: 0.1389,  acc: 0.3466, val_acc: 0.3053\n",
      "Epoch [151/200], loss: 0.5766, val_loss: 0.1388,  acc: 0.3507, val_acc: 0.3111\n",
      "Epoch [152/200], loss: 0.5793, val_loss: 0.1392,  acc: 0.3427, val_acc: 0.3053\n",
      "Epoch [153/200], loss: 0.5793, val_loss: 0.1390,  acc: 0.3456, val_acc: 0.3053\n",
      "Epoch [154/200], loss: 0.5822, val_loss: 0.1388,  acc: 0.3389, val_acc: 0.3053\n",
      "Epoch [155/200], loss: 0.5731, val_loss: 0.1388,  acc: 0.3479, val_acc: 0.3053\n",
      "Epoch [156/200], loss: 0.5848, val_loss: 0.1393,  acc: 0.3417, val_acc: 0.3053\n",
      "Epoch [157/200], loss: 0.5802, val_loss: 0.1387,  acc: 0.3453, val_acc: 0.3111\n",
      "Epoch [158/200], loss: 0.5778, val_loss: 0.1388,  acc: 0.3421, val_acc: 0.3053\n",
      "Epoch [159/200], loss: 0.5851, val_loss: 0.1391,  acc: 0.3458, val_acc: 0.3053\n",
      "Epoch [160/200], loss: 0.5766, val_loss: 0.1388,  acc: 0.3418, val_acc: 0.3111\n",
      "Epoch [161/200], loss: 0.5862, val_loss: 0.1393,  acc: 0.3470, val_acc: 0.3111\n",
      "Epoch [162/200], loss: 0.5806, val_loss: 0.1389,  acc: 0.3420, val_acc: 0.3053\n",
      "Epoch [163/200], loss: 0.5800, val_loss: 0.1387,  acc: 0.3374, val_acc: 0.3111\n",
      "Epoch [164/200], loss: 0.5765, val_loss: 0.1388,  acc: 0.3476, val_acc: 0.3301\n",
      "Epoch [165/200], loss: 0.5809, val_loss: 0.1386,  acc: 0.3457, val_acc: 0.3301\n",
      "Epoch [166/200], loss: 0.5758, val_loss: 0.1385,  acc: 0.3418, val_acc: 0.3111\n",
      "Epoch [167/200], loss: 0.5788, val_loss: 0.1385,  acc: 0.3401, val_acc: 0.3145\n",
      "Epoch [168/200], loss: 0.5784, val_loss: 0.1392,  acc: 0.3494, val_acc: 0.3301\n",
      "Epoch [169/200], loss: 0.5804, val_loss: 0.1389,  acc: 0.3424, val_acc: 0.3111\n",
      "Epoch [170/200], loss: 0.5765, val_loss: 0.1388,  acc: 0.3499, val_acc: 0.3053\n",
      "Epoch [171/200], loss: 0.5783, val_loss: 0.1387,  acc: 0.3458, val_acc: 0.3111\n",
      "Epoch [172/200], loss: 0.5827, val_loss: 0.1392,  acc: 0.3506, val_acc: 0.3111\n",
      "Epoch [173/200], loss: 0.5825, val_loss: 0.1394,  acc: 0.3457, val_acc: 0.2898\n",
      "Epoch [174/200], loss: 0.5787, val_loss: 0.1384,  acc: 0.3368, val_acc: 0.3111\n",
      "Epoch [175/200], loss: 0.5798, val_loss: 0.1388,  acc: 0.3411, val_acc: 0.3053\n",
      "Epoch [176/200], loss: 0.5836, val_loss: 0.1390,  acc: 0.3408, val_acc: 0.3111\n",
      "Epoch [177/200], loss: 0.5809, val_loss: 0.1389,  acc: 0.3440, val_acc: 0.3111\n",
      "Epoch [178/200], loss: 0.5795, val_loss: 0.1388,  acc: 0.3424, val_acc: 0.3053\n",
      "Epoch [179/200], loss: 0.5753, val_loss: 0.1389,  acc: 0.3435, val_acc: 0.3111\n",
      "Epoch [180/200], loss: 0.5801, val_loss: 0.1386,  acc: 0.3428, val_acc: 0.3140\n",
      "Epoch [181/200], loss: 0.5805, val_loss: 0.1390,  acc: 0.3427, val_acc: 0.3111\n",
      "Epoch [182/200], loss: 0.5835, val_loss: 0.1393,  acc: 0.3476, val_acc: 0.3111\n",
      "Epoch [183/200], loss: 0.5818, val_loss: 0.1387,  acc: 0.3388, val_acc: 0.3140\n",
      "Epoch [184/200], loss: 0.5816, val_loss: 0.1393,  acc: 0.3440, val_acc: 0.3053\n",
      "Epoch [185/200], loss: 0.5824, val_loss: 0.1396,  acc: 0.3407, val_acc: 0.3053\n",
      "Epoch [186/200], loss: 0.5781, val_loss: 0.1387,  acc: 0.3385, val_acc: 0.3140\n",
      "Epoch [187/200], loss: 0.5781, val_loss: 0.1387,  acc: 0.3430, val_acc: 0.3111\n",
      "Epoch [188/200], loss: 0.5857, val_loss: 0.1396,  acc: 0.3379, val_acc: 0.2898\n",
      "Epoch [189/200], loss: 0.5826, val_loss: 0.1392,  acc: 0.3479, val_acc: 0.3140\n",
      "Epoch [190/200], loss: 0.5811, val_loss: 0.1396,  acc: 0.3484, val_acc: 0.2898\n",
      "Epoch [191/200], loss: 0.5817, val_loss: 0.1388,  acc: 0.3435, val_acc: 0.3111\n",
      "Epoch [192/200], loss: 0.5808, val_loss: 0.1389,  acc: 0.3477, val_acc: 0.3053\n",
      "Epoch [193/200], loss: 0.5858, val_loss: 0.1393,  acc: 0.3417, val_acc: 0.3301\n",
      "Epoch [194/200], loss: 0.5833, val_loss: 0.1392,  acc: 0.3489, val_acc: 0.3053\n",
      "Epoch [195/200], loss: 0.5797, val_loss: 0.1388,  acc: 0.3422, val_acc: 0.3140\n",
      "Epoch [196/200], loss: 0.5792, val_loss: 0.1388,  acc: 0.3500, val_acc: 0.3140\n",
      "Epoch [197/200], loss: 0.5820, val_loss: 0.1387,  acc: 0.3425, val_acc: 0.3140\n",
      "Epoch [198/200], loss: 0.5831, val_loss: 0.1390,  acc: 0.3453, val_acc: 0.3295\n",
      "Epoch [199/200], loss: 0.5778, val_loss: 0.1384,  acc: 0.3507, val_acc: 0.3140\n",
      "Epoch [200/200], loss: 0.5747, val_loss: 0.1385,  acc: 0.3420, val_acc: 0.3111\n",
      "---------- cv2 ------------\n",
      "Epoch [1/200], loss: 0.7119, val_loss: 0.1808,  acc: 0.5036, val_acc: 0.5037\n",
      "Epoch [2/200], loss: 0.7108, val_loss: 0.1804,  acc: 0.5036, val_acc: 0.5037\n",
      "Epoch [3/200], loss: 0.7088, val_loss: 0.1800,  acc: 0.5036, val_acc: 0.5037\n",
      "Epoch [4/200], loss: 0.7061, val_loss: 0.1792,  acc: 0.5036, val_acc: 0.5037\n",
      "Epoch [5/200], loss: 0.7020, val_loss: 0.1778,  acc: 0.5032, val_acc: 0.5026\n",
      "Epoch [6/200], loss: 0.6952, val_loss: 0.1756,  acc: 0.4738, val_acc: 0.4100\n",
      "Epoch [7/200], loss: 0.6837, val_loss: 0.1719,  acc: 0.4160, val_acc: 0.3341\n",
      "Epoch [8/200], loss: 0.6691, val_loss: 0.1669,  acc: 0.3776, val_acc: 0.2961\n",
      "Epoch [9/200], loss: 0.6462, val_loss: 0.1603,  acc: 0.3609, val_acc: 0.2864\n",
      "Epoch [10/200], loss: 0.6242, val_loss: 0.1548,  acc: 0.3542, val_acc: 0.2858\n",
      "Epoch [11/200], loss: 0.6152, val_loss: 0.1519,  acc: 0.3486, val_acc: 0.2858\n",
      "Epoch [12/200], loss: 0.6108, val_loss: 0.1506,  acc: 0.3525, val_acc: 0.2875\n",
      "Epoch [13/200], loss: 0.6012, val_loss: 0.1495,  acc: 0.3520, val_acc: 0.2875\n",
      "Epoch [14/200], loss: 0.6069, val_loss: 0.1491,  acc: 0.3548, val_acc: 0.2979\n",
      "Epoch [15/200], loss: 0.6013, val_loss: 0.1488,  acc: 0.3548, val_acc: 0.2875\n",
      "Epoch [16/200], loss: 0.6055, val_loss: 0.1486,  acc: 0.3464, val_acc: 0.2984\n",
      "Epoch [17/200], loss: 0.5994, val_loss: 0.1483,  acc: 0.3504, val_acc: 0.2984\n",
      "Epoch [18/200], loss: 0.6036, val_loss: 0.1480,  acc: 0.3453, val_acc: 0.2984\n",
      "Epoch [19/200], loss: 0.5973, val_loss: 0.1475,  acc: 0.3530, val_acc: 0.2984\n",
      "Epoch [20/200], loss: 0.5964, val_loss: 0.1470,  acc: 0.3550, val_acc: 0.2904\n",
      "Epoch [21/200], loss: 0.5979, val_loss: 0.1470,  acc: 0.3520, val_acc: 0.2800\n",
      "Epoch [22/200], loss: 0.5994, val_loss: 0.1467,  acc: 0.3454, val_acc: 0.2904\n",
      "Epoch [23/200], loss: 0.5937, val_loss: 0.1461,  acc: 0.3489, val_acc: 0.2904\n",
      "Epoch [24/200], loss: 0.5921, val_loss: 0.1458,  acc: 0.3483, val_acc: 0.2904\n",
      "Epoch [25/200], loss: 0.5939, val_loss: 0.1457,  acc: 0.3473, val_acc: 0.2904\n",
      "Epoch [26/200], loss: 0.5933, val_loss: 0.1455,  acc: 0.3467, val_acc: 0.2904\n",
      "Epoch [27/200], loss: 0.5914, val_loss: 0.1450,  acc: 0.3450, val_acc: 0.2904\n",
      "Epoch [28/200], loss: 0.5940, val_loss: 0.1450,  acc: 0.3402, val_acc: 0.2904\n",
      "Epoch [29/200], loss: 0.5887, val_loss: 0.1445,  acc: 0.3425, val_acc: 0.2904\n",
      "Epoch [30/200], loss: 0.5885, val_loss: 0.1441,  acc: 0.3502, val_acc: 0.2904\n",
      "Epoch [31/200], loss: 0.5833, val_loss: 0.1438,  acc: 0.3433, val_acc: 0.2904\n",
      "Epoch [32/200], loss: 0.5868, val_loss: 0.1437,  acc: 0.3492, val_acc: 0.2904\n",
      "Epoch [33/200], loss: 0.5872, val_loss: 0.1438,  acc: 0.3448, val_acc: 0.2904\n",
      "Epoch [34/200], loss: 0.5837, val_loss: 0.1434,  acc: 0.3492, val_acc: 0.2904\n",
      "Epoch [35/200], loss: 0.5857, val_loss: 0.1435,  acc: 0.3456, val_acc: 0.2904\n",
      "Epoch [36/200], loss: 0.5846, val_loss: 0.1431,  acc: 0.3453, val_acc: 0.2904\n",
      "Epoch [37/200], loss: 0.5863, val_loss: 0.1431,  acc: 0.3443, val_acc: 0.2904\n",
      "Epoch [38/200], loss: 0.5831, val_loss: 0.1431,  acc: 0.3430, val_acc: 0.2800\n",
      "Epoch [39/200], loss: 0.5809, val_loss: 0.1427,  acc: 0.3457, val_acc: 0.2904\n",
      "Epoch [40/200], loss: 0.5857, val_loss: 0.1432,  acc: 0.3443, val_acc: 0.2795\n",
      "Epoch [41/200], loss: 0.5890, val_loss: 0.1432,  acc: 0.3402, val_acc: 0.2800\n",
      "Epoch [42/200], loss: 0.5836, val_loss: 0.1428,  acc: 0.3418, val_acc: 0.2904\n",
      "Epoch [43/200], loss: 0.5875, val_loss: 0.1429,  acc: 0.3430, val_acc: 0.2904\n",
      "Epoch [44/200], loss: 0.5843, val_loss: 0.1426,  acc: 0.3460, val_acc: 0.2904\n",
      "Epoch [45/200], loss: 0.5841, val_loss: 0.1428,  acc: 0.3422, val_acc: 0.2800\n",
      "Epoch [46/200], loss: 0.5821, val_loss: 0.1426,  acc: 0.3438, val_acc: 0.2800\n",
      "Epoch [47/200], loss: 0.5858, val_loss: 0.1425,  acc: 0.3444, val_acc: 0.2795\n",
      "Epoch [48/200], loss: 0.5798, val_loss: 0.1425,  acc: 0.3422, val_acc: 0.2800\n",
      "Epoch [49/200], loss: 0.5816, val_loss: 0.1424,  acc: 0.3417, val_acc: 0.2921\n",
      "Epoch [50/200], loss: 0.5839, val_loss: 0.1425,  acc: 0.3463, val_acc: 0.2800\n",
      "Epoch [51/200], loss: 0.5833, val_loss: 0.1428,  acc: 0.3421, val_acc: 0.2818\n",
      "Epoch [52/200], loss: 0.5824, val_loss: 0.1424,  acc: 0.3410, val_acc: 0.2800\n",
      "Epoch [53/200], loss: 0.5874, val_loss: 0.1429,  acc: 0.3433, val_acc: 0.2795\n",
      "Epoch [54/200], loss: 0.5808, val_loss: 0.1425,  acc: 0.3460, val_acc: 0.2800\n",
      "Epoch [55/200], loss: 0.5886, val_loss: 0.1426,  acc: 0.3430, val_acc: 0.2818\n",
      "Epoch [56/200], loss: 0.5914, val_loss: 0.1425,  acc: 0.3431, val_acc: 0.2800\n",
      "Epoch [57/200], loss: 0.5810, val_loss: 0.1423,  acc: 0.3421, val_acc: 0.2800\n",
      "Epoch [58/200], loss: 0.5840, val_loss: 0.1423,  acc: 0.3402, val_acc: 0.2800\n",
      "Epoch [59/200], loss: 0.5841, val_loss: 0.1425,  acc: 0.3480, val_acc: 0.2777\n",
      "Epoch [60/200], loss: 0.5819, val_loss: 0.1422,  acc: 0.3391, val_acc: 0.2921\n",
      "Epoch [61/200], loss: 0.5840, val_loss: 0.1422,  acc: 0.3430, val_acc: 0.2795\n",
      "Epoch [62/200], loss: 0.5833, val_loss: 0.1422,  acc: 0.3412, val_acc: 0.2795\n",
      "Epoch [63/200], loss: 0.5826, val_loss: 0.1422,  acc: 0.3494, val_acc: 0.2795\n",
      "Epoch [64/200], loss: 0.5800, val_loss: 0.1422,  acc: 0.3456, val_acc: 0.2795\n",
      "Epoch [65/200], loss: 0.5830, val_loss: 0.1422,  acc: 0.3437, val_acc: 0.2795\n",
      "Epoch [66/200], loss: 0.5845, val_loss: 0.1423,  acc: 0.3430, val_acc: 0.2812\n",
      "Epoch [67/200], loss: 0.5833, val_loss: 0.1421,  acc: 0.3437, val_acc: 0.2795\n",
      "Epoch [68/200], loss: 0.5809, val_loss: 0.1424,  acc: 0.3385, val_acc: 0.2795\n",
      "Epoch [69/200], loss: 0.5791, val_loss: 0.1422,  acc: 0.3421, val_acc: 0.2777\n",
      "Epoch [70/200], loss: 0.5830, val_loss: 0.1423,  acc: 0.3404, val_acc: 0.2812\n",
      "Epoch [71/200], loss: 0.5822, val_loss: 0.1420,  acc: 0.3433, val_acc: 0.2812\n",
      "Epoch [72/200], loss: 0.5774, val_loss: 0.1420,  acc: 0.3417, val_acc: 0.2812\n",
      "Epoch [73/200], loss: 0.5860, val_loss: 0.1426,  acc: 0.3422, val_acc: 0.2795\n",
      "Epoch [74/200], loss: 0.5830, val_loss: 0.1423,  acc: 0.3402, val_acc: 0.2795\n",
      "Epoch [75/200], loss: 0.5775, val_loss: 0.1419,  acc: 0.3476, val_acc: 0.2795\n",
      "Epoch [76/200], loss: 0.5794, val_loss: 0.1421,  acc: 0.3451, val_acc: 0.2795\n",
      "Epoch [77/200], loss: 0.5830, val_loss: 0.1422,  acc: 0.3417, val_acc: 0.2795\n",
      "Epoch [78/200], loss: 0.5799, val_loss: 0.1418,  acc: 0.3435, val_acc: 0.2795\n",
      "Epoch [79/200], loss: 0.5825, val_loss: 0.1418,  acc: 0.3424, val_acc: 0.2795\n",
      "Epoch [80/200], loss: 0.5811, val_loss: 0.1418,  acc: 0.3435, val_acc: 0.2795\n",
      "Epoch [81/200], loss: 0.5814, val_loss: 0.1416,  acc: 0.3481, val_acc: 0.2795\n",
      "Epoch [82/200], loss: 0.5787, val_loss: 0.1417,  acc: 0.3441, val_acc: 0.2795\n",
      "Epoch [83/200], loss: 0.5843, val_loss: 0.1418,  acc: 0.3456, val_acc: 0.2795\n",
      "Epoch [84/200], loss: 0.5841, val_loss: 0.1419,  acc: 0.3451, val_acc: 0.2795\n",
      "Epoch [85/200], loss: 0.5834, val_loss: 0.1417,  acc: 0.3417, val_acc: 0.2795\n",
      "Epoch [86/200], loss: 0.5795, val_loss: 0.1417,  acc: 0.3448, val_acc: 0.2795\n",
      "Epoch [87/200], loss: 0.5787, val_loss: 0.1417,  acc: 0.3489, val_acc: 0.2795\n",
      "Epoch [88/200], loss: 0.5790, val_loss: 0.1418,  acc: 0.3425, val_acc: 0.2795\n",
      "Epoch [89/200], loss: 0.5817, val_loss: 0.1420,  acc: 0.3453, val_acc: 0.2795\n",
      "Epoch [90/200], loss: 0.5825, val_loss: 0.1416,  acc: 0.3460, val_acc: 0.2795\n",
      "Epoch [91/200], loss: 0.5830, val_loss: 0.1421,  acc: 0.3448, val_acc: 0.2795\n",
      "Epoch [92/200], loss: 0.5816, val_loss: 0.1419,  acc: 0.3430, val_acc: 0.2777\n",
      "Epoch [93/200], loss: 0.5811, val_loss: 0.1419,  acc: 0.3418, val_acc: 0.2795\n",
      "Epoch [94/200], loss: 0.5800, val_loss: 0.1419,  acc: 0.3440, val_acc: 0.2795\n",
      "Epoch [95/200], loss: 0.5779, val_loss: 0.1417,  acc: 0.3458, val_acc: 0.2795\n",
      "Epoch [96/200], loss: 0.5819, val_loss: 0.1415,  acc: 0.3422, val_acc: 0.2795\n",
      "Epoch [97/200], loss: 0.5813, val_loss: 0.1415,  acc: 0.3427, val_acc: 0.2875\n",
      "Epoch [98/200], loss: 0.5808, val_loss: 0.1418,  acc: 0.3476, val_acc: 0.2875\n",
      "Epoch [99/200], loss: 0.5738, val_loss: 0.1415,  acc: 0.3477, val_acc: 0.2875\n",
      "Epoch [100/200], loss: 0.5760, val_loss: 0.1413,  acc: 0.3477, val_acc: 0.2875\n",
      "Epoch [101/200], loss: 0.5800, val_loss: 0.1414,  acc: 0.3510, val_acc: 0.2875\n",
      "Epoch [102/200], loss: 0.5821, val_loss: 0.1417,  acc: 0.3476, val_acc: 0.2875\n",
      "Epoch [103/200], loss: 0.5801, val_loss: 0.1415,  acc: 0.3500, val_acc: 0.2875\n",
      "Epoch [104/200], loss: 0.5800, val_loss: 0.1417,  acc: 0.3433, val_acc: 0.2875\n",
      "Epoch [105/200], loss: 0.5820, val_loss: 0.1417,  acc: 0.3458, val_acc: 0.2875\n",
      "Epoch [106/200], loss: 0.5807, val_loss: 0.1416,  acc: 0.3467, val_acc: 0.2875\n",
      "Epoch [107/200], loss: 0.5812, val_loss: 0.1415,  acc: 0.3456, val_acc: 0.2875\n",
      "Epoch [108/200], loss: 0.5812, val_loss: 0.1414,  acc: 0.3463, val_acc: 0.2875\n",
      "Epoch [109/200], loss: 0.5778, val_loss: 0.1415,  acc: 0.3527, val_acc: 0.2875\n",
      "Epoch [110/200], loss: 0.5755, val_loss: 0.1414,  acc: 0.3489, val_acc: 0.2875\n",
      "Epoch [111/200], loss: 0.5797, val_loss: 0.1415,  acc: 0.3477, val_acc: 0.2875\n",
      "Epoch [112/200], loss: 0.5758, val_loss: 0.1415,  acc: 0.3499, val_acc: 0.2875\n",
      "Epoch [113/200], loss: 0.5787, val_loss: 0.1414,  acc: 0.3520, val_acc: 0.2875\n",
      "Epoch [114/200], loss: 0.5763, val_loss: 0.1413,  acc: 0.3471, val_acc: 0.2875\n",
      "Epoch [115/200], loss: 0.5824, val_loss: 0.1417,  acc: 0.3453, val_acc: 0.2875\n",
      "Epoch [116/200], loss: 0.5811, val_loss: 0.1415,  acc: 0.3489, val_acc: 0.2875\n",
      "Epoch [117/200], loss: 0.5764, val_loss: 0.1412,  acc: 0.3476, val_acc: 0.2875\n",
      "Epoch [118/200], loss: 0.5795, val_loss: 0.1418,  acc: 0.3470, val_acc: 0.2875\n",
      "Epoch [119/200], loss: 0.5805, val_loss: 0.1416,  acc: 0.3477, val_acc: 0.2875\n",
      "Epoch [120/200], loss: 0.5778, val_loss: 0.1417,  acc: 0.3438, val_acc: 0.2875\n",
      "Epoch [121/200], loss: 0.5746, val_loss: 0.1415,  acc: 0.3437, val_acc: 0.2875\n",
      "Epoch [122/200], loss: 0.5804, val_loss: 0.1414,  acc: 0.3463, val_acc: 0.2875\n",
      "Epoch [123/200], loss: 0.5793, val_loss: 0.1414,  acc: 0.3441, val_acc: 0.2875\n",
      "Epoch [124/200], loss: 0.5787, val_loss: 0.1415,  acc: 0.3460, val_acc: 0.2875\n",
      "Epoch [125/200], loss: 0.5758, val_loss: 0.1413,  acc: 0.3438, val_acc: 0.2875\n",
      "Epoch [126/200], loss: 0.5827, val_loss: 0.1412,  acc: 0.3438, val_acc: 0.2875\n",
      "Epoch [127/200], loss: 0.5812, val_loss: 0.1414,  acc: 0.3463, val_acc: 0.2875\n",
      "Epoch [128/200], loss: 0.5831, val_loss: 0.1415,  acc: 0.3450, val_acc: 0.2875\n",
      "Epoch [129/200], loss: 0.5790, val_loss: 0.1417,  acc: 0.3435, val_acc: 0.2875\n",
      "Epoch [130/200], loss: 0.5778, val_loss: 0.1414,  acc: 0.3471, val_acc: 0.2875\n",
      "Epoch [131/200], loss: 0.5811, val_loss: 0.1414,  acc: 0.3454, val_acc: 0.2875\n",
      "Epoch [132/200], loss: 0.5807, val_loss: 0.1417,  acc: 0.3456, val_acc: 0.2892\n",
      "Epoch [133/200], loss: 0.5811, val_loss: 0.1412,  acc: 0.3500, val_acc: 0.2892\n",
      "Epoch [134/200], loss: 0.5756, val_loss: 0.1414,  acc: 0.3479, val_acc: 0.2875\n",
      "Epoch [135/200], loss: 0.5782, val_loss: 0.1414,  acc: 0.3470, val_acc: 0.2875\n",
      "Epoch [136/200], loss: 0.5813, val_loss: 0.1416,  acc: 0.3441, val_acc: 0.2875\n",
      "Epoch [137/200], loss: 0.5825, val_loss: 0.1414,  acc: 0.3427, val_acc: 0.2892\n",
      "Epoch [138/200], loss: 0.5794, val_loss: 0.1414,  acc: 0.3460, val_acc: 0.2875\n",
      "Epoch [139/200], loss: 0.5769, val_loss: 0.1412,  acc: 0.3464, val_acc: 0.2875\n",
      "Epoch [140/200], loss: 0.5778, val_loss: 0.1415,  acc: 0.3438, val_acc: 0.2875\n",
      "Epoch [141/200], loss: 0.5820, val_loss: 0.1417,  acc: 0.3424, val_acc: 0.2875\n",
      "Epoch [142/200], loss: 0.5825, val_loss: 0.1415,  acc: 0.3444, val_acc: 0.2892\n",
      "Epoch [143/200], loss: 0.5851, val_loss: 0.1417,  acc: 0.3467, val_acc: 0.2875\n",
      "Epoch [144/200], loss: 0.5836, val_loss: 0.1417,  acc: 0.3469, val_acc: 0.2875\n",
      "Epoch [145/200], loss: 0.5805, val_loss: 0.1413,  acc: 0.3480, val_acc: 0.2875\n",
      "Epoch [146/200], loss: 0.5911, val_loss: 0.1419,  acc: 0.3411, val_acc: 0.2875\n",
      "Epoch [147/200], loss: 0.5773, val_loss: 0.1412,  acc: 0.3464, val_acc: 0.2892\n",
      "Epoch [148/200], loss: 0.5814, val_loss: 0.1414,  acc: 0.3474, val_acc: 0.2875\n",
      "Epoch [149/200], loss: 0.5778, val_loss: 0.1413,  acc: 0.3463, val_acc: 0.2892\n",
      "Epoch [150/200], loss: 0.5812, val_loss: 0.1417,  acc: 0.3431, val_acc: 0.2892\n",
      "Epoch [151/200], loss: 0.5765, val_loss: 0.1411,  acc: 0.3481, val_acc: 0.2892\n",
      "Epoch [152/200], loss: 0.5768, val_loss: 0.1413,  acc: 0.3456, val_acc: 0.2892\n",
      "Epoch [153/200], loss: 0.5748, val_loss: 0.1411,  acc: 0.3461, val_acc: 0.2875\n",
      "Epoch [154/200], loss: 0.5765, val_loss: 0.1412,  acc: 0.3469, val_acc: 0.2892\n",
      "Epoch [155/200], loss: 0.5823, val_loss: 0.1412,  acc: 0.3497, val_acc: 0.2875\n",
      "Epoch [156/200], loss: 0.5734, val_loss: 0.1412,  acc: 0.3481, val_acc: 0.2875\n",
      "Epoch [157/200], loss: 0.5766, val_loss: 0.1411,  acc: 0.3507, val_acc: 0.2875\n",
      "Epoch [158/200], loss: 0.5754, val_loss: 0.1414,  acc: 0.3497, val_acc: 0.2875\n",
      "Epoch [159/200], loss: 0.5734, val_loss: 0.1411,  acc: 0.3481, val_acc: 0.2875\n",
      "Epoch [160/200], loss: 0.5797, val_loss: 0.1411,  acc: 0.3487, val_acc: 0.2875\n",
      "Epoch [161/200], loss: 0.5776, val_loss: 0.1412,  acc: 0.3454, val_acc: 0.2892\n",
      "Epoch [162/200], loss: 0.5747, val_loss: 0.1411,  acc: 0.3516, val_acc: 0.2875\n",
      "Epoch [163/200], loss: 0.5788, val_loss: 0.1413,  acc: 0.3483, val_acc: 0.2875\n",
      "Epoch [164/200], loss: 0.5824, val_loss: 0.1414,  acc: 0.3473, val_acc: 0.2875\n",
      "Epoch [165/200], loss: 0.5798, val_loss: 0.1413,  acc: 0.3476, val_acc: 0.2875\n",
      "Epoch [166/200], loss: 0.5763, val_loss: 0.1410,  acc: 0.3486, val_acc: 0.2875\n",
      "Epoch [167/200], loss: 0.5803, val_loss: 0.1414,  acc: 0.3447, val_acc: 0.2892\n",
      "Epoch [168/200], loss: 0.5773, val_loss: 0.1414,  acc: 0.3496, val_acc: 0.2875\n",
      "Epoch [169/200], loss: 0.5803, val_loss: 0.1413,  acc: 0.3430, val_acc: 0.2892\n",
      "Epoch [170/200], loss: 0.5774, val_loss: 0.1412,  acc: 0.3486, val_acc: 0.2892\n",
      "Epoch [171/200], loss: 0.5809, val_loss: 0.1414,  acc: 0.3483, val_acc: 0.2875\n",
      "Epoch [172/200], loss: 0.5800, val_loss: 0.1413,  acc: 0.3450, val_acc: 0.2892\n",
      "Epoch [173/200], loss: 0.5809, val_loss: 0.1411,  acc: 0.3484, val_acc: 0.2892\n",
      "Epoch [174/200], loss: 0.5833, val_loss: 0.1412,  acc: 0.3466, val_acc: 0.2875\n",
      "Epoch [175/200], loss: 0.5809, val_loss: 0.1412,  acc: 0.3457, val_acc: 0.2875\n",
      "Epoch [176/200], loss: 0.5782, val_loss: 0.1410,  acc: 0.3509, val_acc: 0.2875\n",
      "Epoch [177/200], loss: 0.5786, val_loss: 0.1411,  acc: 0.3480, val_acc: 0.2875\n",
      "Epoch [178/200], loss: 0.5824, val_loss: 0.1414,  acc: 0.3443, val_acc: 0.2875\n",
      "Epoch [179/200], loss: 0.5837, val_loss: 0.1414,  acc: 0.3530, val_acc: 0.2875\n",
      "Epoch [180/200], loss: 0.5782, val_loss: 0.1413,  acc: 0.3425, val_acc: 0.2875\n",
      "Epoch [181/200], loss: 0.5797, val_loss: 0.1414,  acc: 0.3430, val_acc: 0.2875\n",
      "Epoch [182/200], loss: 0.5774, val_loss: 0.1414,  acc: 0.3447, val_acc: 0.2875\n",
      "Epoch [183/200], loss: 0.5800, val_loss: 0.1413,  acc: 0.3457, val_acc: 0.2875\n",
      "Epoch [184/200], loss: 0.5766, val_loss: 0.1410,  acc: 0.3469, val_acc: 0.2875\n",
      "Epoch [185/200], loss: 0.5814, val_loss: 0.1412,  acc: 0.3454, val_acc: 0.2875\n",
      "Epoch [186/200], loss: 0.5776, val_loss: 0.1414,  acc: 0.3428, val_acc: 0.2875\n",
      "Epoch [187/200], loss: 0.5793, val_loss: 0.1412,  acc: 0.3471, val_acc: 0.2875\n",
      "Epoch [188/200], loss: 0.5729, val_loss: 0.1411,  acc: 0.3444, val_acc: 0.2858\n",
      "Epoch [189/200], loss: 0.5819, val_loss: 0.1412,  acc: 0.3418, val_acc: 0.2858\n",
      "Epoch [190/200], loss: 0.5783, val_loss: 0.1412,  acc: 0.3453, val_acc: 0.2892\n",
      "Epoch [191/200], loss: 0.5777, val_loss: 0.1413,  acc: 0.3464, val_acc: 0.2875\n",
      "Epoch [192/200], loss: 0.5793, val_loss: 0.1412,  acc: 0.3454, val_acc: 0.2875\n",
      "Epoch [193/200], loss: 0.5790, val_loss: 0.1416,  acc: 0.3484, val_acc: 0.2875\n",
      "Epoch [194/200], loss: 0.5805, val_loss: 0.1415,  acc: 0.3451, val_acc: 0.2875\n",
      "Epoch [195/200], loss: 0.5818, val_loss: 0.1412,  acc: 0.3489, val_acc: 0.2875\n",
      "Epoch [196/200], loss: 0.5823, val_loss: 0.1416,  acc: 0.3484, val_acc: 0.2875\n",
      "Epoch [197/200], loss: 0.5771, val_loss: 0.1411,  acc: 0.3463, val_acc: 0.2875\n",
      "Epoch [198/200], loss: 0.5786, val_loss: 0.1413,  acc: 0.3441, val_acc: 0.2875\n",
      "Epoch [199/200], loss: 0.5726, val_loss: 0.1413,  acc: 0.3483, val_acc: 0.2875\n",
      "Epoch [200/200], loss: 0.5809, val_loss: 0.1414,  acc: 0.3440, val_acc: 0.2875\n",
      "---------- cv3 ------------\n",
      "Epoch [1/200], loss: 0.7154, val_loss: 0.1810,  acc: 0.4971, val_acc: 0.5014\n",
      "Epoch [2/200], loss: 0.7117, val_loss: 0.1804,  acc: 0.4945, val_acc: 0.5014\n",
      "Epoch [3/200], loss: 0.7090, val_loss: 0.1797,  acc: 0.4715, val_acc: 0.5009\n",
      "Epoch [4/200], loss: 0.7071, val_loss: 0.1790,  acc: 0.4513, val_acc: 0.4738\n",
      "Epoch [5/200], loss: 0.7044, val_loss: 0.1781,  acc: 0.3881, val_acc: 0.4503\n",
      "Epoch [6/200], loss: 0.7009, val_loss: 0.1770,  acc: 0.3742, val_acc: 0.4135\n",
      "Epoch [7/200], loss: 0.6935, val_loss: 0.1743,  acc: 0.3831, val_acc: 0.3876\n",
      "Epoch [8/200], loss: 0.6799, val_loss: 0.1694,  acc: 0.4084, val_acc: 0.3790\n",
      "Epoch [9/200], loss: 0.6552, val_loss: 0.1612,  acc: 0.4032, val_acc: 0.3721\n",
      "Epoch [10/200], loss: 0.6295, val_loss: 0.1541,  acc: 0.3828, val_acc: 0.3548\n",
      "Epoch [11/200], loss: 0.6162, val_loss: 0.1501,  acc: 0.3661, val_acc: 0.3329\n",
      "Epoch [12/200], loss: 0.6017, val_loss: 0.1480,  acc: 0.3664, val_acc: 0.3053\n",
      "Epoch [13/200], loss: 0.6020, val_loss: 0.1469,  acc: 0.3585, val_acc: 0.3030\n",
      "Epoch [14/200], loss: 0.5992, val_loss: 0.1463,  acc: 0.3578, val_acc: 0.3030\n",
      "Epoch [15/200], loss: 0.6030, val_loss: 0.1461,  acc: 0.3517, val_acc: 0.3030\n",
      "Epoch [16/200], loss: 0.5931, val_loss: 0.1456,  acc: 0.3502, val_acc: 0.3088\n",
      "Epoch [17/200], loss: 0.5947, val_loss: 0.1451,  acc: 0.3494, val_acc: 0.3030\n",
      "Epoch [18/200], loss: 0.5985, val_loss: 0.1452,  acc: 0.3457, val_acc: 0.3030\n",
      "Epoch [19/200], loss: 0.5952, val_loss: 0.1449,  acc: 0.3492, val_acc: 0.3030\n",
      "Epoch [20/200], loss: 0.5987, val_loss: 0.1446,  acc: 0.3496, val_acc: 0.3088\n",
      "Epoch [21/200], loss: 0.5951, val_loss: 0.1443,  acc: 0.3510, val_acc: 0.3111\n",
      "Epoch [22/200], loss: 0.5895, val_loss: 0.1440,  acc: 0.3504, val_acc: 0.3111\n",
      "Epoch [23/200], loss: 0.5873, val_loss: 0.1436,  acc: 0.3492, val_acc: 0.3036\n",
      "Epoch [24/200], loss: 0.5870, val_loss: 0.1435,  acc: 0.3517, val_acc: 0.3013\n",
      "Epoch [25/200], loss: 0.5905, val_loss: 0.1433,  acc: 0.3492, val_acc: 0.3036\n",
      "Epoch [26/200], loss: 0.5851, val_loss: 0.1432,  acc: 0.3515, val_acc: 0.2904\n",
      "Epoch [27/200], loss: 0.5875, val_loss: 0.1431,  acc: 0.3473, val_acc: 0.2956\n",
      "Epoch [28/200], loss: 0.5830, val_loss: 0.1428,  acc: 0.3480, val_acc: 0.2956\n",
      "Epoch [29/200], loss: 0.5856, val_loss: 0.1428,  acc: 0.3476, val_acc: 0.2956\n",
      "Epoch [30/200], loss: 0.5875, val_loss: 0.1429,  acc: 0.3513, val_acc: 0.2979\n",
      "Epoch [31/200], loss: 0.5887, val_loss: 0.1427,  acc: 0.3424, val_acc: 0.2979\n",
      "Epoch [32/200], loss: 0.5801, val_loss: 0.1426,  acc: 0.3469, val_acc: 0.3036\n",
      "Epoch [33/200], loss: 0.5859, val_loss: 0.1425,  acc: 0.3438, val_acc: 0.2927\n",
      "Epoch [34/200], loss: 0.5863, val_loss: 0.1425,  acc: 0.3461, val_acc: 0.2927\n",
      "Epoch [35/200], loss: 0.5829, val_loss: 0.1423,  acc: 0.3433, val_acc: 0.2979\n",
      "Epoch [36/200], loss: 0.5822, val_loss: 0.1423,  acc: 0.3497, val_acc: 0.2904\n",
      "Epoch [37/200], loss: 0.5883, val_loss: 0.1425,  acc: 0.3392, val_acc: 0.2979\n",
      "Epoch [38/200], loss: 0.5830, val_loss: 0.1423,  acc: 0.3434, val_acc: 0.2904\n",
      "Epoch [39/200], loss: 0.5839, val_loss: 0.1421,  acc: 0.3410, val_acc: 0.2904\n",
      "Epoch [40/200], loss: 0.5839, val_loss: 0.1421,  acc: 0.3428, val_acc: 0.2904\n",
      "Epoch [41/200], loss: 0.5893, val_loss: 0.1421,  acc: 0.3441, val_acc: 0.2927\n",
      "Epoch [42/200], loss: 0.5860, val_loss: 0.1420,  acc: 0.3448, val_acc: 0.2927\n",
      "Epoch [43/200], loss: 0.5877, val_loss: 0.1421,  acc: 0.3410, val_acc: 0.3053\n",
      "Epoch [44/200], loss: 0.5829, val_loss: 0.1420,  acc: 0.3456, val_acc: 0.2904\n",
      "Epoch [45/200], loss: 0.5848, val_loss: 0.1419,  acc: 0.3382, val_acc: 0.3002\n",
      "Epoch [46/200], loss: 0.5817, val_loss: 0.1417,  acc: 0.3414, val_acc: 0.3053\n",
      "Epoch [47/200], loss: 0.5850, val_loss: 0.1416,  acc: 0.3437, val_acc: 0.3002\n",
      "Epoch [48/200], loss: 0.5790, val_loss: 0.1414,  acc: 0.3470, val_acc: 0.3002\n",
      "Epoch [49/200], loss: 0.5873, val_loss: 0.1417,  acc: 0.3464, val_acc: 0.3002\n",
      "Epoch [50/200], loss: 0.5844, val_loss: 0.1418,  acc: 0.3484, val_acc: 0.3002\n",
      "Epoch [51/200], loss: 0.5804, val_loss: 0.1415,  acc: 0.3451, val_acc: 0.3002\n",
      "Epoch [52/200], loss: 0.5815, val_loss: 0.1416,  acc: 0.3433, val_acc: 0.2904\n",
      "Epoch [53/200], loss: 0.5841, val_loss: 0.1416,  acc: 0.3333, val_acc: 0.2979\n",
      "Epoch [54/200], loss: 0.5832, val_loss: 0.1416,  acc: 0.3414, val_acc: 0.2979\n",
      "Epoch [55/200], loss: 0.5807, val_loss: 0.1413,  acc: 0.3411, val_acc: 0.2979\n",
      "Epoch [56/200], loss: 0.5762, val_loss: 0.1411,  acc: 0.3454, val_acc: 0.2979\n",
      "Epoch [57/200], loss: 0.5830, val_loss: 0.1416,  acc: 0.3364, val_acc: 0.3059\n",
      "Epoch [58/200], loss: 0.5795, val_loss: 0.1413,  acc: 0.3470, val_acc: 0.3002\n",
      "Epoch [59/200], loss: 0.5832, val_loss: 0.1414,  acc: 0.3382, val_acc: 0.2927\n",
      "Epoch [60/200], loss: 0.5826, val_loss: 0.1413,  acc: 0.3389, val_acc: 0.2927\n",
      "Epoch [61/200], loss: 0.5814, val_loss: 0.1411,  acc: 0.3471, val_acc: 0.2927\n",
      "Epoch [62/200], loss: 0.5872, val_loss: 0.1414,  acc: 0.3418, val_acc: 0.2760\n",
      "Epoch [63/200], loss: 0.5866, val_loss: 0.1414,  acc: 0.3345, val_acc: 0.2927\n",
      "Epoch [64/200], loss: 0.5818, val_loss: 0.1412,  acc: 0.3408, val_acc: 0.2927\n",
      "Epoch [65/200], loss: 0.5789, val_loss: 0.1411,  acc: 0.3362, val_acc: 0.2927\n",
      "Epoch [66/200], loss: 0.5814, val_loss: 0.1413,  acc: 0.3492, val_acc: 0.2927\n",
      "Epoch [67/200], loss: 0.5822, val_loss: 0.1412,  acc: 0.3447, val_acc: 0.2927\n",
      "Epoch [68/200], loss: 0.5773, val_loss: 0.1413,  acc: 0.3364, val_acc: 0.3111\n",
      "Epoch [69/200], loss: 0.5796, val_loss: 0.1411,  acc: 0.3458, val_acc: 0.3002\n",
      "Epoch [70/200], loss: 0.5826, val_loss: 0.1411,  acc: 0.3441, val_acc: 0.2760\n",
      "Epoch [71/200], loss: 0.5786, val_loss: 0.1409,  acc: 0.3379, val_acc: 0.3082\n",
      "Epoch [72/200], loss: 0.5873, val_loss: 0.1413,  acc: 0.3415, val_acc: 0.2927\n",
      "Epoch [73/200], loss: 0.5792, val_loss: 0.1412,  acc: 0.3359, val_acc: 0.2927\n",
      "Epoch [74/200], loss: 0.5844, val_loss: 0.1412,  acc: 0.3410, val_acc: 0.2927\n",
      "Epoch [75/200], loss: 0.5772, val_loss: 0.1411,  acc: 0.3368, val_acc: 0.2927\n",
      "Epoch [76/200], loss: 0.5765, val_loss: 0.1409,  acc: 0.3480, val_acc: 0.2927\n",
      "Epoch [77/200], loss: 0.5817, val_loss: 0.1411,  acc: 0.3420, val_acc: 0.2927\n",
      "Epoch [78/200], loss: 0.5796, val_loss: 0.1409,  acc: 0.3371, val_acc: 0.2760\n",
      "Epoch [79/200], loss: 0.5803, val_loss: 0.1411,  acc: 0.3330, val_acc: 0.2927\n",
      "Epoch [80/200], loss: 0.5838, val_loss: 0.1412,  acc: 0.3389, val_acc: 0.2927\n",
      "Epoch [81/200], loss: 0.5847, val_loss: 0.1412,  acc: 0.3352, val_acc: 0.2927\n",
      "Epoch [82/200], loss: 0.5798, val_loss: 0.1410,  acc: 0.3454, val_acc: 0.2927\n",
      "Epoch [83/200], loss: 0.5808, val_loss: 0.1411,  acc: 0.3358, val_acc: 0.2927\n",
      "Epoch [84/200], loss: 0.5830, val_loss: 0.1409,  acc: 0.3450, val_acc: 0.2927\n",
      "Epoch [85/200], loss: 0.5828, val_loss: 0.1411,  acc: 0.3398, val_acc: 0.2927\n",
      "Epoch [86/200], loss: 0.5794, val_loss: 0.1412,  acc: 0.3379, val_acc: 0.2927\n",
      "Epoch [87/200], loss: 0.5833, val_loss: 0.1411,  acc: 0.3351, val_acc: 0.2927\n",
      "Epoch [88/200], loss: 0.5799, val_loss: 0.1409,  acc: 0.3412, val_acc: 0.2927\n",
      "Epoch [89/200], loss: 0.5763, val_loss: 0.1408,  acc: 0.3411, val_acc: 0.2927\n",
      "Epoch [90/200], loss: 0.5821, val_loss: 0.1412,  acc: 0.3457, val_acc: 0.3059\n",
      "Epoch [91/200], loss: 0.5842, val_loss: 0.1413,  acc: 0.3389, val_acc: 0.3059\n",
      "Epoch [92/200], loss: 0.5782, val_loss: 0.1410,  acc: 0.3435, val_acc: 0.2927\n",
      "Epoch [93/200], loss: 0.5773, val_loss: 0.1409,  acc: 0.3385, val_acc: 0.2984\n",
      "Epoch [94/200], loss: 0.5781, val_loss: 0.1409,  acc: 0.3437, val_acc: 0.2984\n",
      "Epoch [95/200], loss: 0.5786, val_loss: 0.1408,  acc: 0.3398, val_acc: 0.2760\n",
      "Epoch [96/200], loss: 0.5821, val_loss: 0.1409,  acc: 0.3387, val_acc: 0.2984\n",
      "Epoch [97/200], loss: 0.5793, val_loss: 0.1408,  acc: 0.3414, val_acc: 0.2927\n",
      "Epoch [98/200], loss: 0.5814, val_loss: 0.1409,  acc: 0.3427, val_acc: 0.2927\n",
      "Epoch [99/200], loss: 0.5800, val_loss: 0.1409,  acc: 0.3411, val_acc: 0.2984\n",
      "Epoch [100/200], loss: 0.5800, val_loss: 0.1407,  acc: 0.3469, val_acc: 0.2927\n",
      "Epoch [101/200], loss: 0.5812, val_loss: 0.1409,  acc: 0.3425, val_acc: 0.2984\n",
      "Epoch [102/200], loss: 0.5780, val_loss: 0.1408,  acc: 0.3434, val_acc: 0.2984\n",
      "Epoch [103/200], loss: 0.5823, val_loss: 0.1408,  acc: 0.3424, val_acc: 0.2927\n",
      "Epoch [104/200], loss: 0.5775, val_loss: 0.1409,  acc: 0.3402, val_acc: 0.2927\n",
      "Epoch [105/200], loss: 0.5809, val_loss: 0.1409,  acc: 0.3394, val_acc: 0.2927\n",
      "Epoch [106/200], loss: 0.5770, val_loss: 0.1407,  acc: 0.3447, val_acc: 0.2927\n",
      "Epoch [107/200], loss: 0.5843, val_loss: 0.1410,  acc: 0.3421, val_acc: 0.2927\n",
      "Epoch [108/200], loss: 0.5779, val_loss: 0.1407,  acc: 0.3431, val_acc: 0.2927\n",
      "Epoch [109/200], loss: 0.5788, val_loss: 0.1409,  acc: 0.3384, val_acc: 0.2927\n",
      "Epoch [110/200], loss: 0.5819, val_loss: 0.1412,  acc: 0.3443, val_acc: 0.2927\n",
      "Epoch [111/200], loss: 0.5800, val_loss: 0.1409,  acc: 0.3387, val_acc: 0.2927\n",
      "Epoch [112/200], loss: 0.5778, val_loss: 0.1407,  acc: 0.3431, val_acc: 0.3059\n",
      "Epoch [113/200], loss: 0.5815, val_loss: 0.1409,  acc: 0.3443, val_acc: 0.3059\n",
      "Epoch [114/200], loss: 0.5808, val_loss: 0.1409,  acc: 0.3430, val_acc: 0.3059\n",
      "Epoch [115/200], loss: 0.5875, val_loss: 0.1410,  acc: 0.3394, val_acc: 0.3059\n",
      "Epoch [116/200], loss: 0.5805, val_loss: 0.1408,  acc: 0.3454, val_acc: 0.2984\n",
      "Epoch [117/200], loss: 0.5792, val_loss: 0.1408,  acc: 0.3454, val_acc: 0.2984\n",
      "Epoch [118/200], loss: 0.5826, val_loss: 0.1408,  acc: 0.3444, val_acc: 0.3059\n",
      "Epoch [119/200], loss: 0.5782, val_loss: 0.1408,  acc: 0.3467, val_acc: 0.2927\n",
      "Epoch [120/200], loss: 0.5758, val_loss: 0.1407,  acc: 0.3392, val_acc: 0.3002\n",
      "Epoch [121/200], loss: 0.5766, val_loss: 0.1405,  acc: 0.3476, val_acc: 0.3059\n",
      "Epoch [122/200], loss: 0.5781, val_loss: 0.1407,  acc: 0.3430, val_acc: 0.2927\n",
      "Epoch [123/200], loss: 0.5799, val_loss: 0.1409,  acc: 0.3407, val_acc: 0.3059\n",
      "Epoch [124/200], loss: 0.5778, val_loss: 0.1409,  acc: 0.3445, val_acc: 0.3059\n",
      "Epoch [125/200], loss: 0.5797, val_loss: 0.1407,  acc: 0.3410, val_acc: 0.3059\n",
      "Epoch [126/200], loss: 0.5774, val_loss: 0.1405,  acc: 0.3437, val_acc: 0.3002\n",
      "Epoch [127/200], loss: 0.5781, val_loss: 0.1406,  acc: 0.3384, val_acc: 0.3059\n",
      "Epoch [128/200], loss: 0.5797, val_loss: 0.1405,  acc: 0.3473, val_acc: 0.2984\n",
      "Epoch [129/200], loss: 0.5772, val_loss: 0.1409,  acc: 0.3402, val_acc: 0.2984\n",
      "Epoch [130/200], loss: 0.5794, val_loss: 0.1408,  acc: 0.3399, val_acc: 0.2984\n",
      "Epoch [131/200], loss: 0.5765, val_loss: 0.1407,  acc: 0.3437, val_acc: 0.2927\n",
      "Epoch [132/200], loss: 0.5773, val_loss: 0.1407,  acc: 0.3411, val_acc: 0.2927\n",
      "Epoch [133/200], loss: 0.5790, val_loss: 0.1407,  acc: 0.3425, val_acc: 0.2927\n",
      "Epoch [134/200], loss: 0.5721, val_loss: 0.1405,  acc: 0.3422, val_acc: 0.2927\n",
      "Epoch [135/200], loss: 0.5792, val_loss: 0.1406,  acc: 0.3401, val_acc: 0.2984\n",
      "Epoch [136/200], loss: 0.5773, val_loss: 0.1406,  acc: 0.3405, val_acc: 0.2984\n",
      "Epoch [137/200], loss: 0.5778, val_loss: 0.1407,  acc: 0.3453, val_acc: 0.2984\n",
      "Epoch [138/200], loss: 0.5790, val_loss: 0.1405,  acc: 0.3428, val_acc: 0.2927\n",
      "Epoch [139/200], loss: 0.5800, val_loss: 0.1406,  acc: 0.3397, val_acc: 0.2927\n",
      "Epoch [140/200], loss: 0.5761, val_loss: 0.1407,  acc: 0.3414, val_acc: 0.2984\n",
      "Epoch [141/200], loss: 0.5789, val_loss: 0.1407,  acc: 0.3392, val_acc: 0.2927\n",
      "Epoch [142/200], loss: 0.5819, val_loss: 0.1407,  acc: 0.3402, val_acc: 0.2927\n",
      "Epoch [143/200], loss: 0.5770, val_loss: 0.1405,  acc: 0.3392, val_acc: 0.2927\n",
      "Epoch [144/200], loss: 0.5780, val_loss: 0.1406,  acc: 0.3438, val_acc: 0.2927\n",
      "Epoch [145/200], loss: 0.5777, val_loss: 0.1408,  acc: 0.3425, val_acc: 0.3278\n",
      "Epoch [146/200], loss: 0.5787, val_loss: 0.1405,  acc: 0.3443, val_acc: 0.2984\n",
      "Epoch [147/200], loss: 0.5777, val_loss: 0.1407,  acc: 0.3444, val_acc: 0.2984\n",
      "Epoch [148/200], loss: 0.5788, val_loss: 0.1406,  acc: 0.3444, val_acc: 0.2984\n",
      "Epoch [149/200], loss: 0.5838, val_loss: 0.1408,  acc: 0.3402, val_acc: 0.3059\n",
      "Epoch [150/200], loss: 0.5780, val_loss: 0.1406,  acc: 0.3444, val_acc: 0.2927\n",
      "Epoch [151/200], loss: 0.5840, val_loss: 0.1410,  acc: 0.3376, val_acc: 0.2760\n",
      "Epoch [152/200], loss: 0.5845, val_loss: 0.1409,  acc: 0.3399, val_acc: 0.2984\n",
      "Epoch [153/200], loss: 0.5808, val_loss: 0.1411,  acc: 0.3343, val_acc: 0.3059\n",
      "Epoch [154/200], loss: 0.5787, val_loss: 0.1408,  acc: 0.3408, val_acc: 0.2984\n",
      "Epoch [155/200], loss: 0.5810, val_loss: 0.1407,  acc: 0.3389, val_acc: 0.2984\n",
      "Epoch [156/200], loss: 0.5804, val_loss: 0.1408,  acc: 0.3440, val_acc: 0.2984\n",
      "Epoch [157/200], loss: 0.5766, val_loss: 0.1410,  acc: 0.3437, val_acc: 0.3059\n",
      "Epoch [158/200], loss: 0.5794, val_loss: 0.1410,  acc: 0.3427, val_acc: 0.3059\n",
      "Epoch [159/200], loss: 0.5797, val_loss: 0.1408,  acc: 0.3415, val_acc: 0.2984\n",
      "Epoch [160/200], loss: 0.5760, val_loss: 0.1406,  acc: 0.3421, val_acc: 0.2984\n",
      "Epoch [161/200], loss: 0.5801, val_loss: 0.1406,  acc: 0.3405, val_acc: 0.2984\n",
      "Epoch [162/200], loss: 0.5773, val_loss: 0.1404,  acc: 0.3402, val_acc: 0.2984\n",
      "Epoch [163/200], loss: 0.5770, val_loss: 0.1405,  acc: 0.3374, val_acc: 0.2984\n",
      "Epoch [164/200], loss: 0.5739, val_loss: 0.1405,  acc: 0.3420, val_acc: 0.2984\n",
      "Epoch [165/200], loss: 0.5742, val_loss: 0.1404,  acc: 0.3397, val_acc: 0.2984\n",
      "Epoch [166/200], loss: 0.5813, val_loss: 0.1408,  acc: 0.3381, val_acc: 0.2984\n",
      "Epoch [167/200], loss: 0.5776, val_loss: 0.1405,  acc: 0.3415, val_acc: 0.2984\n",
      "Epoch [168/200], loss: 0.5814, val_loss: 0.1406,  acc: 0.3431, val_acc: 0.2984\n",
      "Epoch [169/200], loss: 0.5771, val_loss: 0.1408,  acc: 0.3394, val_acc: 0.2984\n",
      "Epoch [170/200], loss: 0.5791, val_loss: 0.1408,  acc: 0.3411, val_acc: 0.2984\n",
      "Epoch [171/200], loss: 0.5785, val_loss: 0.1409,  acc: 0.3395, val_acc: 0.2984\n",
      "Epoch [172/200], loss: 0.5769, val_loss: 0.1406,  acc: 0.3401, val_acc: 0.2984\n",
      "Epoch [173/200], loss: 0.5818, val_loss: 0.1406,  acc: 0.3388, val_acc: 0.2984\n",
      "Epoch [174/200], loss: 0.5742, val_loss: 0.1403,  acc: 0.3402, val_acc: 0.2984\n",
      "Epoch [175/200], loss: 0.5820, val_loss: 0.1406,  acc: 0.3353, val_acc: 0.2984\n",
      "Epoch [176/200], loss: 0.5753, val_loss: 0.1405,  acc: 0.3441, val_acc: 0.2984\n",
      "Epoch [177/200], loss: 0.5813, val_loss: 0.1406,  acc: 0.3372, val_acc: 0.2984\n",
      "Epoch [178/200], loss: 0.5783, val_loss: 0.1405,  acc: 0.3394, val_acc: 0.3059\n",
      "Epoch [179/200], loss: 0.5766, val_loss: 0.1406,  acc: 0.3339, val_acc: 0.2892\n",
      "Epoch [180/200], loss: 0.5798, val_loss: 0.1407,  acc: 0.3376, val_acc: 0.3059\n",
      "Epoch [181/200], loss: 0.5764, val_loss: 0.1405,  acc: 0.3451, val_acc: 0.2984\n",
      "Epoch [182/200], loss: 0.5810, val_loss: 0.1405,  acc: 0.3408, val_acc: 0.3059\n",
      "Epoch [183/200], loss: 0.5767, val_loss: 0.1404,  acc: 0.3422, val_acc: 0.2984\n",
      "Epoch [184/200], loss: 0.5814, val_loss: 0.1406,  acc: 0.3411, val_acc: 0.3059\n",
      "Epoch [185/200], loss: 0.5851, val_loss: 0.1410,  acc: 0.3365, val_acc: 0.3059\n",
      "Epoch [186/200], loss: 0.5777, val_loss: 0.1409,  acc: 0.3421, val_acc: 0.3059\n",
      "Epoch [187/200], loss: 0.5797, val_loss: 0.1406,  acc: 0.3457, val_acc: 0.3059\n",
      "Epoch [188/200], loss: 0.5781, val_loss: 0.1406,  acc: 0.3428, val_acc: 0.3059\n",
      "Epoch [189/200], loss: 0.5784, val_loss: 0.1405,  acc: 0.3422, val_acc: 0.3059\n",
      "Epoch [190/200], loss: 0.5832, val_loss: 0.1407,  acc: 0.3445, val_acc: 0.3059\n",
      "Epoch [191/200], loss: 0.5738, val_loss: 0.1408,  acc: 0.3450, val_acc: 0.3059\n",
      "Epoch [192/200], loss: 0.5776, val_loss: 0.1405,  acc: 0.3425, val_acc: 0.3059\n",
      "Epoch [193/200], loss: 0.5747, val_loss: 0.1405,  acc: 0.3457, val_acc: 0.3059\n",
      "Epoch [194/200], loss: 0.5819, val_loss: 0.1406,  acc: 0.3443, val_acc: 0.3059\n",
      "Epoch [195/200], loss: 0.5790, val_loss: 0.1406,  acc: 0.3453, val_acc: 0.3059\n",
      "Epoch [196/200], loss: 0.5778, val_loss: 0.1406,  acc: 0.3470, val_acc: 0.3059\n",
      "Epoch [197/200], loss: 0.5796, val_loss: 0.1407,  acc: 0.3394, val_acc: 0.3059\n",
      "Epoch [198/200], loss: 0.5761, val_loss: 0.1404,  acc: 0.3407, val_acc: 0.3059\n",
      "Epoch [199/200], loss: 0.5752, val_loss: 0.1405,  acc: 0.3463, val_acc: 0.3059\n",
      "Epoch [200/200], loss: 0.5796, val_loss: 0.1406,  acc: 0.3450, val_acc: 0.3059\n",
      "---------- cv4 ------------\n",
      "Epoch [1/200], loss: 0.7182, val_loss: 0.1817,  acc: 0.5019, val_acc: 0.5081\n",
      "Epoch [2/200], loss: 0.7134, val_loss: 0.1805,  acc: 0.5018, val_acc: 0.5081\n",
      "Epoch [3/200], loss: 0.7079, val_loss: 0.1786,  acc: 0.4994, val_acc: 0.5081\n",
      "Epoch [4/200], loss: 0.7000, val_loss: 0.1760,  acc: 0.4723, val_acc: 0.4868\n",
      "Epoch [5/200], loss: 0.6897, val_loss: 0.1723,  acc: 0.4115, val_acc: 0.3556\n",
      "Epoch [6/200], loss: 0.6748, val_loss: 0.1673,  acc: 0.3892, val_acc: 0.3550\n",
      "Epoch [7/200], loss: 0.6571, val_loss: 0.1615,  acc: 0.3725, val_acc: 0.3470\n",
      "Epoch [8/200], loss: 0.6400, val_loss: 0.1562,  acc: 0.3649, val_acc: 0.3441\n",
      "Epoch [9/200], loss: 0.6280, val_loss: 0.1518,  acc: 0.3572, val_acc: 0.3320\n",
      "Epoch [10/200], loss: 0.6230, val_loss: 0.1488,  acc: 0.3503, val_acc: 0.3193\n",
      "Epoch [11/200], loss: 0.6132, val_loss: 0.1462,  acc: 0.3474, val_acc: 0.3211\n",
      "Epoch [12/200], loss: 0.6099, val_loss: 0.1446,  acc: 0.3482, val_acc: 0.3211\n",
      "Epoch [13/200], loss: 0.6050, val_loss: 0.1434,  acc: 0.3472, val_acc: 0.3211\n",
      "Epoch [14/200], loss: 0.6070, val_loss: 0.1429,  acc: 0.3477, val_acc: 0.3216\n",
      "Epoch [15/200], loss: 0.6006, val_loss: 0.1425,  acc: 0.3464, val_acc: 0.3216\n",
      "Epoch [16/200], loss: 0.6049, val_loss: 0.1422,  acc: 0.3442, val_acc: 0.3216\n",
      "Epoch [17/200], loss: 0.6013, val_loss: 0.1418,  acc: 0.3468, val_acc: 0.3216\n",
      "Epoch [18/200], loss: 0.6042, val_loss: 0.1416,  acc: 0.3467, val_acc: 0.3216\n",
      "Epoch [19/200], loss: 0.6009, val_loss: 0.1415,  acc: 0.3449, val_acc: 0.3216\n",
      "Epoch [20/200], loss: 0.5963, val_loss: 0.1411,  acc: 0.3458, val_acc: 0.3216\n",
      "Epoch [21/200], loss: 0.6032, val_loss: 0.1414,  acc: 0.3441, val_acc: 0.3159\n",
      "Epoch [22/200], loss: 0.6020, val_loss: 0.1409,  acc: 0.3468, val_acc: 0.3159\n",
      "Epoch [23/200], loss: 0.6015, val_loss: 0.1408,  acc: 0.3418, val_acc: 0.3216\n",
      "Epoch [24/200], loss: 0.6018, val_loss: 0.1407,  acc: 0.3418, val_acc: 0.3188\n",
      "Epoch [25/200], loss: 0.5967, val_loss: 0.1402,  acc: 0.3428, val_acc: 0.3188\n",
      "Epoch [26/200], loss: 0.5933, val_loss: 0.1402,  acc: 0.3431, val_acc: 0.3159\n",
      "Epoch [27/200], loss: 0.5938, val_loss: 0.1396,  acc: 0.3465, val_acc: 0.3188\n",
      "Epoch [28/200], loss: 0.5954, val_loss: 0.1398,  acc: 0.3382, val_acc: 0.3188\n",
      "Epoch [29/200], loss: 0.5941, val_loss: 0.1397,  acc: 0.3393, val_acc: 0.3188\n",
      "Epoch [30/200], loss: 0.5981, val_loss: 0.1398,  acc: 0.3398, val_acc: 0.3188\n",
      "Epoch [31/200], loss: 0.5952, val_loss: 0.1394,  acc: 0.3356, val_acc: 0.3188\n",
      "Epoch [32/200], loss: 0.5935, val_loss: 0.1393,  acc: 0.3472, val_acc: 0.3159\n",
      "Epoch [33/200], loss: 0.5933, val_loss: 0.1387,  acc: 0.3399, val_acc: 0.3188\n",
      "Epoch [34/200], loss: 0.5898, val_loss: 0.1386,  acc: 0.3398, val_acc: 0.3188\n",
      "Epoch [35/200], loss: 0.5907, val_loss: 0.1382,  acc: 0.3441, val_acc: 0.3188\n",
      "Epoch [36/200], loss: 0.5916, val_loss: 0.1384,  acc: 0.3415, val_acc: 0.3188\n",
      "Epoch [37/200], loss: 0.5889, val_loss: 0.1381,  acc: 0.3405, val_acc: 0.3228\n",
      "Epoch [38/200], loss: 0.5927, val_loss: 0.1385,  acc: 0.3412, val_acc: 0.3228\n",
      "Epoch [39/200], loss: 0.5911, val_loss: 0.1379,  acc: 0.3421, val_acc: 0.3188\n",
      "Epoch [40/200], loss: 0.5889, val_loss: 0.1378,  acc: 0.3449, val_acc: 0.3188\n",
      "Epoch [41/200], loss: 0.5836, val_loss: 0.1377,  acc: 0.3441, val_acc: 0.3188\n",
      "Epoch [42/200], loss: 0.5929, val_loss: 0.1379,  acc: 0.3375, val_acc: 0.3228\n",
      "Epoch [43/200], loss: 0.5919, val_loss: 0.1380,  acc: 0.3379, val_acc: 0.3090\n",
      "Epoch [44/200], loss: 0.5832, val_loss: 0.1375,  acc: 0.3434, val_acc: 0.3228\n",
      "Epoch [45/200], loss: 0.5842, val_loss: 0.1373,  acc: 0.3402, val_acc: 0.3188\n",
      "Epoch [46/200], loss: 0.5915, val_loss: 0.1379,  acc: 0.3400, val_acc: 0.3188\n",
      "Epoch [47/200], loss: 0.5858, val_loss: 0.1376,  acc: 0.3429, val_acc: 0.3228\n",
      "Epoch [48/200], loss: 0.5904, val_loss: 0.1372,  acc: 0.3412, val_acc: 0.3130\n",
      "Epoch [49/200], loss: 0.5808, val_loss: 0.1373,  acc: 0.3412, val_acc: 0.3130\n",
      "Epoch [50/200], loss: 0.5873, val_loss: 0.1371,  acc: 0.3423, val_acc: 0.3188\n",
      "Epoch [51/200], loss: 0.5896, val_loss: 0.1373,  acc: 0.3409, val_acc: 0.3228\n",
      "Epoch [52/200], loss: 0.5879, val_loss: 0.1373,  acc: 0.3405, val_acc: 0.3303\n",
      "Epoch [53/200], loss: 0.5908, val_loss: 0.1374,  acc: 0.3429, val_acc: 0.3303\n",
      "Epoch [54/200], loss: 0.5861, val_loss: 0.1372,  acc: 0.3400, val_acc: 0.3205\n",
      "Epoch [55/200], loss: 0.5896, val_loss: 0.1373,  acc: 0.3455, val_acc: 0.3090\n",
      "Epoch [56/200], loss: 0.5883, val_loss: 0.1373,  acc: 0.3387, val_acc: 0.3228\n",
      "Epoch [57/200], loss: 0.5906, val_loss: 0.1371,  acc: 0.3436, val_acc: 0.3090\n",
      "Epoch [58/200], loss: 0.5829, val_loss: 0.1369,  acc: 0.3399, val_acc: 0.3228\n",
      "Epoch [59/200], loss: 0.5841, val_loss: 0.1369,  acc: 0.3457, val_acc: 0.3090\n",
      "Epoch [60/200], loss: 0.5840, val_loss: 0.1368,  acc: 0.3395, val_acc: 0.3228\n",
      "Epoch [61/200], loss: 0.5869, val_loss: 0.1370,  acc: 0.3423, val_acc: 0.3130\n",
      "Epoch [62/200], loss: 0.5902, val_loss: 0.1369,  acc: 0.3392, val_acc: 0.3130\n",
      "Epoch [63/200], loss: 0.5862, val_loss: 0.1371,  acc: 0.3431, val_acc: 0.3188\n",
      "Epoch [64/200], loss: 0.5892, val_loss: 0.1368,  acc: 0.3454, val_acc: 0.3188\n",
      "Epoch [65/200], loss: 0.5882, val_loss: 0.1371,  acc: 0.3403, val_acc: 0.3228\n",
      "Epoch [66/200], loss: 0.5884, val_loss: 0.1368,  acc: 0.3390, val_acc: 0.3090\n",
      "Epoch [67/200], loss: 0.5844, val_loss: 0.1366,  acc: 0.3400, val_acc: 0.3228\n",
      "Epoch [68/200], loss: 0.5886, val_loss: 0.1372,  acc: 0.3441, val_acc: 0.3090\n",
      "Epoch [69/200], loss: 0.5845, val_loss: 0.1371,  acc: 0.3426, val_acc: 0.3090\n",
      "Epoch [70/200], loss: 0.5851, val_loss: 0.1371,  acc: 0.3434, val_acc: 0.3090\n",
      "Epoch [71/200], loss: 0.5857, val_loss: 0.1369,  acc: 0.3445, val_acc: 0.3090\n",
      "Epoch [72/200], loss: 0.5854, val_loss: 0.1371,  acc: 0.3395, val_acc: 0.3285\n",
      "Epoch [73/200], loss: 0.5863, val_loss: 0.1369,  acc: 0.3432, val_acc: 0.3090\n",
      "Epoch [74/200], loss: 0.5867, val_loss: 0.1370,  acc: 0.3385, val_acc: 0.3090\n",
      "Epoch [75/200], loss: 0.5832, val_loss: 0.1369,  acc: 0.3399, val_acc: 0.3090\n",
      "Epoch [76/200], loss: 0.5855, val_loss: 0.1370,  acc: 0.3413, val_acc: 0.3090\n",
      "Epoch [77/200], loss: 0.5886, val_loss: 0.1371,  acc: 0.3409, val_acc: 0.3090\n",
      "Epoch [78/200], loss: 0.5857, val_loss: 0.1369,  acc: 0.3406, val_acc: 0.3090\n",
      "Epoch [79/200], loss: 0.5854, val_loss: 0.1368,  acc: 0.3418, val_acc: 0.3090\n",
      "Epoch [80/200], loss: 0.5818, val_loss: 0.1366,  acc: 0.3402, val_acc: 0.3147\n",
      "Epoch [81/200], loss: 0.5849, val_loss: 0.1369,  acc: 0.3375, val_acc: 0.3119\n",
      "Epoch [82/200], loss: 0.5808, val_loss: 0.1367,  acc: 0.3412, val_acc: 0.3090\n",
      "Epoch [83/200], loss: 0.5836, val_loss: 0.1370,  acc: 0.3367, val_acc: 0.3147\n",
      "Epoch [84/200], loss: 0.5797, val_loss: 0.1363,  acc: 0.3484, val_acc: 0.3147\n",
      "Epoch [85/200], loss: 0.5849, val_loss: 0.1365,  acc: 0.3442, val_acc: 0.3320\n",
      "Epoch [86/200], loss: 0.5866, val_loss: 0.1368,  acc: 0.3410, val_acc: 0.3147\n",
      "Epoch [87/200], loss: 0.5847, val_loss: 0.1369,  acc: 0.3405, val_acc: 0.3147\n",
      "Epoch [88/200], loss: 0.5838, val_loss: 0.1367,  acc: 0.3435, val_acc: 0.3147\n",
      "Epoch [89/200], loss: 0.5822, val_loss: 0.1368,  acc: 0.3423, val_acc: 0.3147\n",
      "Epoch [90/200], loss: 0.5861, val_loss: 0.1369,  acc: 0.3387, val_acc: 0.3147\n",
      "Epoch [91/200], loss: 0.5882, val_loss: 0.1372,  acc: 0.3406, val_acc: 0.3147\n",
      "Epoch [92/200], loss: 0.5845, val_loss: 0.1368,  acc: 0.3396, val_acc: 0.3147\n",
      "Epoch [93/200], loss: 0.5824, val_loss: 0.1370,  acc: 0.3421, val_acc: 0.3147\n",
      "Epoch [94/200], loss: 0.5842, val_loss: 0.1369,  acc: 0.3432, val_acc: 0.3119\n",
      "Epoch [95/200], loss: 0.5820, val_loss: 0.1368,  acc: 0.3375, val_acc: 0.3147\n",
      "Epoch [96/200], loss: 0.5853, val_loss: 0.1368,  acc: 0.3448, val_acc: 0.3147\n",
      "Epoch [97/200], loss: 0.5870, val_loss: 0.1371,  acc: 0.3380, val_acc: 0.3147\n",
      "Epoch [98/200], loss: 0.5868, val_loss: 0.1371,  acc: 0.3393, val_acc: 0.3147\n",
      "Epoch [99/200], loss: 0.5838, val_loss: 0.1372,  acc: 0.3410, val_acc: 0.3119\n",
      "Epoch [100/200], loss: 0.5855, val_loss: 0.1370,  acc: 0.3367, val_acc: 0.3119\n",
      "Epoch [101/200], loss: 0.5822, val_loss: 0.1366,  acc: 0.3422, val_acc: 0.3147\n",
      "Epoch [102/200], loss: 0.5850, val_loss: 0.1372,  acc: 0.3331, val_acc: 0.3147\n",
      "Epoch [103/200], loss: 0.5834, val_loss: 0.1369,  acc: 0.3403, val_acc: 0.3147\n",
      "Epoch [104/200], loss: 0.5808, val_loss: 0.1368,  acc: 0.3387, val_acc: 0.3147\n",
      "Epoch [105/200], loss: 0.5880, val_loss: 0.1370,  acc: 0.3428, val_acc: 0.3119\n",
      "Epoch [106/200], loss: 0.5842, val_loss: 0.1369,  acc: 0.3389, val_acc: 0.3147\n",
      "Epoch [107/200], loss: 0.5866, val_loss: 0.1370,  acc: 0.3432, val_acc: 0.3119\n",
      "Epoch [108/200], loss: 0.5828, val_loss: 0.1368,  acc: 0.3439, val_acc: 0.3119\n",
      "Epoch [109/200], loss: 0.5837, val_loss: 0.1367,  acc: 0.3367, val_acc: 0.3147\n",
      "Epoch [110/200], loss: 0.5805, val_loss: 0.1370,  acc: 0.3421, val_acc: 0.3119\n",
      "Epoch [111/200], loss: 0.5817, val_loss: 0.1366,  acc: 0.3405, val_acc: 0.3119\n",
      "Epoch [112/200], loss: 0.5846, val_loss: 0.1371,  acc: 0.3382, val_acc: 0.3119\n",
      "Epoch [113/200], loss: 0.5854, val_loss: 0.1370,  acc: 0.3434, val_acc: 0.3119\n",
      "Epoch [114/200], loss: 0.5821, val_loss: 0.1369,  acc: 0.3367, val_acc: 0.3119\n",
      "Epoch [115/200], loss: 0.5793, val_loss: 0.1367,  acc: 0.3410, val_acc: 0.3119\n",
      "Epoch [116/200], loss: 0.5829, val_loss: 0.1367,  acc: 0.3402, val_acc: 0.3119\n",
      "Epoch [117/200], loss: 0.5859, val_loss: 0.1371,  acc: 0.3403, val_acc: 0.3119\n",
      "Epoch [118/200], loss: 0.5810, val_loss: 0.1367,  acc: 0.3400, val_acc: 0.3119\n",
      "Epoch [119/200], loss: 0.5800, val_loss: 0.1368,  acc: 0.3438, val_acc: 0.3119\n",
      "Epoch [120/200], loss: 0.5811, val_loss: 0.1369,  acc: 0.3400, val_acc: 0.3119\n",
      "Epoch [121/200], loss: 0.5856, val_loss: 0.1370,  acc: 0.3352, val_acc: 0.3159\n",
      "Epoch [122/200], loss: 0.5834, val_loss: 0.1370,  acc: 0.3431, val_acc: 0.3119\n",
      "Epoch [123/200], loss: 0.5798, val_loss: 0.1365,  acc: 0.3352, val_acc: 0.3119\n",
      "Epoch [124/200], loss: 0.5847, val_loss: 0.1369,  acc: 0.3398, val_acc: 0.3119\n",
      "Epoch [125/200], loss: 0.5799, val_loss: 0.1370,  acc: 0.3400, val_acc: 0.3159\n",
      "Epoch [126/200], loss: 0.5831, val_loss: 0.1370,  acc: 0.3390, val_acc: 0.3119\n",
      "Epoch [127/200], loss: 0.5824, val_loss: 0.1368,  acc: 0.3385, val_acc: 0.3119\n",
      "Epoch [128/200], loss: 0.5821, val_loss: 0.1367,  acc: 0.3360, val_acc: 0.3119\n",
      "Epoch [129/200], loss: 0.5862, val_loss: 0.1368,  acc: 0.3382, val_acc: 0.3119\n",
      "Epoch [130/200], loss: 0.5792, val_loss: 0.1370,  acc: 0.3451, val_acc: 0.2894\n",
      "Epoch [131/200], loss: 0.5819, val_loss: 0.1369,  acc: 0.3403, val_acc: 0.3119\n",
      "Epoch [132/200], loss: 0.5827, val_loss: 0.1371,  acc: 0.3389, val_acc: 0.3119\n",
      "Epoch [133/200], loss: 0.5833, val_loss: 0.1370,  acc: 0.3367, val_acc: 0.3119\n",
      "Epoch [134/200], loss: 0.5840, val_loss: 0.1372,  acc: 0.3402, val_acc: 0.3119\n",
      "Epoch [135/200], loss: 0.5818, val_loss: 0.1367,  acc: 0.3455, val_acc: 0.3119\n",
      "Epoch [136/200], loss: 0.5832, val_loss: 0.1371,  acc: 0.3367, val_acc: 0.2894\n",
      "Epoch [137/200], loss: 0.5813, val_loss: 0.1369,  acc: 0.3426, val_acc: 0.2894\n",
      "Epoch [138/200], loss: 0.5803, val_loss: 0.1368,  acc: 0.3429, val_acc: 0.3159\n",
      "Epoch [139/200], loss: 0.5819, val_loss: 0.1367,  acc: 0.3376, val_acc: 0.2894\n",
      "Epoch [140/200], loss: 0.5819, val_loss: 0.1366,  acc: 0.3434, val_acc: 0.3159\n",
      "Epoch [141/200], loss: 0.5840, val_loss: 0.1369,  acc: 0.3434, val_acc: 0.3159\n",
      "Epoch [142/200], loss: 0.5813, val_loss: 0.1369,  acc: 0.3431, val_acc: 0.3119\n",
      "Epoch [143/200], loss: 0.5833, val_loss: 0.1368,  acc: 0.3421, val_acc: 0.3159\n",
      "Epoch [144/200], loss: 0.5863, val_loss: 0.1369,  acc: 0.3421, val_acc: 0.3159\n",
      "Epoch [145/200], loss: 0.5889, val_loss: 0.1371,  acc: 0.3441, val_acc: 0.3159\n",
      "Epoch [146/200], loss: 0.5831, val_loss: 0.1368,  acc: 0.3426, val_acc: 0.2934\n",
      "Epoch [147/200], loss: 0.5838, val_loss: 0.1369,  acc: 0.3410, val_acc: 0.2934\n",
      "Epoch [148/200], loss: 0.5762, val_loss: 0.1365,  acc: 0.3392, val_acc: 0.2894\n",
      "Epoch [149/200], loss: 0.5815, val_loss: 0.1369,  acc: 0.3442, val_acc: 0.3159\n",
      "Epoch [150/200], loss: 0.5843, val_loss: 0.1368,  acc: 0.3387, val_acc: 0.3159\n",
      "Epoch [151/200], loss: 0.5806, val_loss: 0.1368,  acc: 0.3390, val_acc: 0.2894\n",
      "Epoch [152/200], loss: 0.5818, val_loss: 0.1367,  acc: 0.3390, val_acc: 0.2894\n",
      "Epoch [153/200], loss: 0.5826, val_loss: 0.1368,  acc: 0.3431, val_acc: 0.3159\n",
      "Epoch [154/200], loss: 0.5789, val_loss: 0.1365,  acc: 0.3405, val_acc: 0.3159\n",
      "Epoch [155/200], loss: 0.5767, val_loss: 0.1368,  acc: 0.3455, val_acc: 0.3119\n",
      "Epoch [156/200], loss: 0.5856, val_loss: 0.1371,  acc: 0.3369, val_acc: 0.3159\n",
      "Epoch [157/200], loss: 0.5863, val_loss: 0.1371,  acc: 0.3356, val_acc: 0.3119\n",
      "Epoch [158/200], loss: 0.5837, val_loss: 0.1371,  acc: 0.3402, val_acc: 0.2894\n",
      "Epoch [159/200], loss: 0.5789, val_loss: 0.1369,  acc: 0.3421, val_acc: 0.3119\n",
      "Epoch [160/200], loss: 0.5838, val_loss: 0.1371,  acc: 0.3375, val_acc: 0.2894\n",
      "Epoch [161/200], loss: 0.5810, val_loss: 0.1366,  acc: 0.3392, val_acc: 0.3119\n",
      "Epoch [162/200], loss: 0.5768, val_loss: 0.1365,  acc: 0.3400, val_acc: 0.3119\n",
      "Epoch [163/200], loss: 0.5806, val_loss: 0.1371,  acc: 0.3408, val_acc: 0.2894\n",
      "Epoch [164/200], loss: 0.5786, val_loss: 0.1367,  acc: 0.3412, val_acc: 0.3159\n",
      "Epoch [165/200], loss: 0.5861, val_loss: 0.1366,  acc: 0.3392, val_acc: 0.3159\n",
      "Epoch [166/200], loss: 0.5841, val_loss: 0.1370,  acc: 0.3385, val_acc: 0.3159\n",
      "Epoch [167/200], loss: 0.5782, val_loss: 0.1367,  acc: 0.3436, val_acc: 0.3159\n",
      "Epoch [168/200], loss: 0.5874, val_loss: 0.1370,  acc: 0.3387, val_acc: 0.3159\n",
      "Epoch [169/200], loss: 0.5791, val_loss: 0.1367,  acc: 0.3454, val_acc: 0.3159\n",
      "Epoch [170/200], loss: 0.5861, val_loss: 0.1369,  acc: 0.3399, val_acc: 0.2894\n",
      "Epoch [171/200], loss: 0.5814, val_loss: 0.1367,  acc: 0.3385, val_acc: 0.3119\n",
      "Epoch [172/200], loss: 0.5777, val_loss: 0.1367,  acc: 0.3408, val_acc: 0.3119\n",
      "Epoch [173/200], loss: 0.5843, val_loss: 0.1369,  acc: 0.3353, val_acc: 0.2894\n",
      "Epoch [174/200], loss: 0.5810, val_loss: 0.1367,  acc: 0.3405, val_acc: 0.3159\n",
      "Epoch [175/200], loss: 0.5873, val_loss: 0.1370,  acc: 0.3392, val_acc: 0.3119\n",
      "Epoch [176/200], loss: 0.5848, val_loss: 0.1372,  acc: 0.3383, val_acc: 0.2894\n",
      "Epoch [177/200], loss: 0.5791, val_loss: 0.1363,  acc: 0.3387, val_acc: 0.2934\n",
      "Epoch [178/200], loss: 0.5824, val_loss: 0.1367,  acc: 0.3436, val_acc: 0.3159\n",
      "Epoch [179/200], loss: 0.5849, val_loss: 0.1370,  acc: 0.3393, val_acc: 0.2894\n",
      "Epoch [180/200], loss: 0.5785, val_loss: 0.1364,  acc: 0.3403, val_acc: 0.3119\n",
      "Epoch [181/200], loss: 0.5844, val_loss: 0.1372,  acc: 0.3317, val_acc: 0.3188\n",
      "Epoch [182/200], loss: 0.5786, val_loss: 0.1367,  acc: 0.3472, val_acc: 0.3188\n",
      "Epoch [183/200], loss: 0.5811, val_loss: 0.1368,  acc: 0.3400, val_acc: 0.3188\n",
      "Epoch [184/200], loss: 0.5752, val_loss: 0.1366,  acc: 0.3505, val_acc: 0.2934\n",
      "Epoch [185/200], loss: 0.5777, val_loss: 0.1364,  acc: 0.3352, val_acc: 0.2934\n",
      "Epoch [186/200], loss: 0.5808, val_loss: 0.1367,  acc: 0.3436, val_acc: 0.2934\n",
      "Epoch [187/200], loss: 0.5864, val_loss: 0.1374,  acc: 0.3408, val_acc: 0.2894\n",
      "Epoch [188/200], loss: 0.5808, val_loss: 0.1367,  acc: 0.3399, val_acc: 0.3159\n",
      "Epoch [189/200], loss: 0.5800, val_loss: 0.1364,  acc: 0.3475, val_acc: 0.3119\n",
      "Epoch [190/200], loss: 0.5802, val_loss: 0.1367,  acc: 0.3373, val_acc: 0.2894\n",
      "Epoch [191/200], loss: 0.5824, val_loss: 0.1367,  acc: 0.3415, val_acc: 0.2934\n",
      "Epoch [192/200], loss: 0.5824, val_loss: 0.1367,  acc: 0.3409, val_acc: 0.2894\n",
      "Epoch [193/200], loss: 0.5823, val_loss: 0.1367,  acc: 0.3353, val_acc: 0.3188\n",
      "Epoch [194/200], loss: 0.5816, val_loss: 0.1370,  acc: 0.3422, val_acc: 0.2963\n",
      "Epoch [195/200], loss: 0.5794, val_loss: 0.1367,  acc: 0.3403, val_acc: 0.2963\n",
      "Epoch [196/200], loss: 0.5817, val_loss: 0.1369,  acc: 0.3405, val_acc: 0.2963\n",
      "Epoch [197/200], loss: 0.5824, val_loss: 0.1367,  acc: 0.3350, val_acc: 0.2934\n",
      "Epoch [198/200], loss: 0.5787, val_loss: 0.1367,  acc: 0.3423, val_acc: 0.2963\n",
      "Epoch [199/200], loss: 0.5824, val_loss: 0.1367,  acc: 0.3395, val_acc: 0.3188\n",
      "Epoch [200/200], loss: 0.5820, val_loss: 0.1367,  acc: 0.3422, val_acc: 0.3188\n",
      "---------- cv5 ------------\n",
      "Epoch [1/200], loss: 0.7064, val_loss: 0.1791,  acc: 0.4751, val_acc: 0.4988\n",
      "Epoch [2/200], loss: 0.7012, val_loss: 0.1781,  acc: 0.4712, val_acc: 0.4971\n",
      "Epoch [3/200], loss: 0.6974, val_loss: 0.1766,  acc: 0.4590, val_acc: 0.4954\n",
      "Epoch [4/200], loss: 0.6912, val_loss: 0.1745,  acc: 0.4328, val_acc: 0.4120\n",
      "Epoch [5/200], loss: 0.6815, val_loss: 0.1716,  acc: 0.4001, val_acc: 0.3625\n",
      "Epoch [6/200], loss: 0.6697, val_loss: 0.1672,  acc: 0.3804, val_acc: 0.3423\n",
      "Epoch [7/200], loss: 0.6512, val_loss: 0.1617,  acc: 0.3779, val_acc: 0.3360\n",
      "Epoch [8/200], loss: 0.6341, val_loss: 0.1567,  acc: 0.3658, val_acc: 0.3239\n",
      "Epoch [9/200], loss: 0.6202, val_loss: 0.1531,  acc: 0.3653, val_acc: 0.3032\n",
      "Epoch [10/200], loss: 0.6089, val_loss: 0.1509,  acc: 0.3592, val_acc: 0.3021\n",
      "Epoch [11/200], loss: 0.6069, val_loss: 0.1495,  acc: 0.3613, val_acc: 0.2980\n",
      "Epoch [12/200], loss: 0.6041, val_loss: 0.1487,  acc: 0.3605, val_acc: 0.2980\n",
      "Epoch [13/200], loss: 0.6082, val_loss: 0.1483,  acc: 0.3561, val_acc: 0.3021\n",
      "Epoch [14/200], loss: 0.6033, val_loss: 0.1480,  acc: 0.3583, val_acc: 0.3061\n",
      "Epoch [15/200], loss: 0.6006, val_loss: 0.1476,  acc: 0.3561, val_acc: 0.2963\n",
      "Epoch [16/200], loss: 0.5998, val_loss: 0.1474,  acc: 0.3557, val_acc: 0.2963\n",
      "Epoch [17/200], loss: 0.5997, val_loss: 0.1471,  acc: 0.3569, val_acc: 0.3003\n",
      "Epoch [18/200], loss: 0.5955, val_loss: 0.1469,  acc: 0.3599, val_acc: 0.3003\n",
      "Epoch [19/200], loss: 0.6028, val_loss: 0.1469,  acc: 0.3536, val_acc: 0.3003\n",
      "Epoch [20/200], loss: 0.5988, val_loss: 0.1466,  acc: 0.3553, val_acc: 0.3009\n",
      "Epoch [21/200], loss: 0.5973, val_loss: 0.1463,  acc: 0.3550, val_acc: 0.2992\n",
      "Epoch [22/200], loss: 0.5983, val_loss: 0.1461,  acc: 0.3505, val_acc: 0.3009\n",
      "Epoch [23/200], loss: 0.5865, val_loss: 0.1456,  acc: 0.3592, val_acc: 0.2992\n",
      "Epoch [24/200], loss: 0.5926, val_loss: 0.1452,  acc: 0.3553, val_acc: 0.2992\n",
      "Epoch [25/200], loss: 0.5923, val_loss: 0.1451,  acc: 0.3515, val_acc: 0.3003\n",
      "Epoch [26/200], loss: 0.5951, val_loss: 0.1450,  acc: 0.3534, val_acc: 0.2992\n",
      "Epoch [27/200], loss: 0.5865, val_loss: 0.1447,  acc: 0.3510, val_acc: 0.2992\n",
      "Epoch [28/200], loss: 0.5890, val_loss: 0.1447,  acc: 0.3490, val_acc: 0.3205\n",
      "Epoch [29/200], loss: 0.5884, val_loss: 0.1443,  acc: 0.3547, val_acc: 0.3009\n",
      "Epoch [30/200], loss: 0.5865, val_loss: 0.1442,  acc: 0.3518, val_acc: 0.3009\n",
      "Epoch [31/200], loss: 0.5899, val_loss: 0.1441,  acc: 0.3524, val_acc: 0.3009\n",
      "Epoch [32/200], loss: 0.5899, val_loss: 0.1437,  acc: 0.3530, val_acc: 0.3009\n",
      "Epoch [33/200], loss: 0.5872, val_loss: 0.1437,  acc: 0.3557, val_acc: 0.2992\n",
      "Epoch [34/200], loss: 0.5862, val_loss: 0.1434,  acc: 0.3528, val_acc: 0.3003\n",
      "Epoch [35/200], loss: 0.5831, val_loss: 0.1434,  acc: 0.3517, val_acc: 0.3003\n",
      "Epoch [36/200], loss: 0.5839, val_loss: 0.1433,  acc: 0.3488, val_acc: 0.3009\n",
      "Epoch [37/200], loss: 0.5841, val_loss: 0.1432,  acc: 0.3527, val_acc: 0.3009\n",
      "Epoch [38/200], loss: 0.5854, val_loss: 0.1433,  acc: 0.3520, val_acc: 0.3228\n",
      "Epoch [39/200], loss: 0.5865, val_loss: 0.1430,  acc: 0.3513, val_acc: 0.3009\n",
      "Epoch [40/200], loss: 0.5826, val_loss: 0.1430,  acc: 0.3487, val_acc: 0.3009\n",
      "Epoch [41/200], loss: 0.5813, val_loss: 0.1429,  acc: 0.3501, val_acc: 0.3003\n",
      "Epoch [42/200], loss: 0.5831, val_loss: 0.1427,  acc: 0.3511, val_acc: 0.2992\n",
      "Epoch [43/200], loss: 0.5849, val_loss: 0.1429,  acc: 0.3477, val_acc: 0.2883\n",
      "Epoch [44/200], loss: 0.5813, val_loss: 0.1428,  acc: 0.3454, val_acc: 0.3003\n",
      "Epoch [45/200], loss: 0.5866, val_loss: 0.1428,  acc: 0.3495, val_acc: 0.2992\n",
      "Epoch [46/200], loss: 0.5816, val_loss: 0.1426,  acc: 0.3507, val_acc: 0.2883\n",
      "Epoch [47/200], loss: 0.5833, val_loss: 0.1426,  acc: 0.3465, val_acc: 0.2883\n",
      "Epoch [48/200], loss: 0.5788, val_loss: 0.1427,  acc: 0.3504, val_acc: 0.2894\n",
      "Epoch [49/200], loss: 0.5814, val_loss: 0.1427,  acc: 0.3457, val_acc: 0.3026\n",
      "Epoch [50/200], loss: 0.5824, val_loss: 0.1426,  acc: 0.3462, val_acc: 0.3026\n",
      "Epoch [51/200], loss: 0.5824, val_loss: 0.1426,  acc: 0.3488, val_acc: 0.2883\n",
      "Epoch [52/200], loss: 0.5812, val_loss: 0.1425,  acc: 0.3477, val_acc: 0.3003\n",
      "Epoch [53/200], loss: 0.5824, val_loss: 0.1424,  acc: 0.3462, val_acc: 0.2883\n",
      "Epoch [54/200], loss: 0.5816, val_loss: 0.1426,  acc: 0.3452, val_acc: 0.2883\n",
      "Epoch [55/200], loss: 0.5796, val_loss: 0.1427,  acc: 0.3431, val_acc: 0.3222\n",
      "Epoch [56/200], loss: 0.5803, val_loss: 0.1425,  acc: 0.3505, val_acc: 0.2883\n",
      "Epoch [57/200], loss: 0.5797, val_loss: 0.1425,  acc: 0.3444, val_acc: 0.3222\n",
      "Epoch [58/200], loss: 0.5804, val_loss: 0.1422,  acc: 0.3533, val_acc: 0.2917\n",
      "Epoch [59/200], loss: 0.5792, val_loss: 0.1424,  acc: 0.3442, val_acc: 0.2917\n",
      "Epoch [60/200], loss: 0.5801, val_loss: 0.1424,  acc: 0.3455, val_acc: 0.2917\n",
      "Epoch [61/200], loss: 0.5777, val_loss: 0.1423,  acc: 0.3480, val_acc: 0.2917\n",
      "Epoch [62/200], loss: 0.5800, val_loss: 0.1424,  acc: 0.3482, val_acc: 0.2842\n",
      "Epoch [63/200], loss: 0.5830, val_loss: 0.1424,  acc: 0.3459, val_acc: 0.2917\n",
      "Epoch [64/200], loss: 0.5810, val_loss: 0.1421,  acc: 0.3495, val_acc: 0.2883\n",
      "Epoch [65/200], loss: 0.5794, val_loss: 0.1423,  acc: 0.3471, val_acc: 0.2883\n",
      "Epoch [66/200], loss: 0.5796, val_loss: 0.1422,  acc: 0.3469, val_acc: 0.2842\n",
      "Epoch [67/200], loss: 0.5796, val_loss: 0.1422,  acc: 0.3402, val_acc: 0.2917\n",
      "Epoch [68/200], loss: 0.5810, val_loss: 0.1423,  acc: 0.3418, val_acc: 0.2883\n",
      "Epoch [69/200], loss: 0.5814, val_loss: 0.1420,  acc: 0.3508, val_acc: 0.2883\n",
      "Epoch [70/200], loss: 0.5781, val_loss: 0.1420,  acc: 0.3435, val_acc: 0.2883\n",
      "Epoch [71/200], loss: 0.5813, val_loss: 0.1420,  acc: 0.3462, val_acc: 0.2906\n",
      "Epoch [72/200], loss: 0.5796, val_loss: 0.1419,  acc: 0.3439, val_acc: 0.2883\n",
      "Epoch [73/200], loss: 0.5793, val_loss: 0.1420,  acc: 0.3452, val_acc: 0.3101\n",
      "Epoch [74/200], loss: 0.5820, val_loss: 0.1420,  acc: 0.3471, val_acc: 0.2883\n",
      "Epoch [75/200], loss: 0.5832, val_loss: 0.1421,  acc: 0.3452, val_acc: 0.2906\n",
      "Epoch [76/200], loss: 0.5796, val_loss: 0.1422,  acc: 0.3457, val_acc: 0.3101\n",
      "Epoch [77/200], loss: 0.5790, val_loss: 0.1419,  acc: 0.3484, val_acc: 0.2883\n",
      "Epoch [78/200], loss: 0.5771, val_loss: 0.1418,  acc: 0.3478, val_acc: 0.2883\n",
      "Epoch [79/200], loss: 0.5802, val_loss: 0.1421,  acc: 0.3436, val_acc: 0.3107\n",
      "Epoch [80/200], loss: 0.5805, val_loss: 0.1422,  acc: 0.3474, val_acc: 0.3113\n",
      "Epoch [81/200], loss: 0.5744, val_loss: 0.1418,  acc: 0.3517, val_acc: 0.2883\n",
      "Epoch [82/200], loss: 0.5782, val_loss: 0.1420,  acc: 0.3498, val_acc: 0.2883\n",
      "Epoch [83/200], loss: 0.5790, val_loss: 0.1419,  acc: 0.3471, val_acc: 0.2906\n",
      "Epoch [84/200], loss: 0.5820, val_loss: 0.1420,  acc: 0.3500, val_acc: 0.2906\n",
      "Epoch [85/200], loss: 0.5784, val_loss: 0.1420,  acc: 0.3444, val_acc: 0.2883\n",
      "Epoch [86/200], loss: 0.5797, val_loss: 0.1419,  acc: 0.3468, val_acc: 0.2693\n",
      "Epoch [87/200], loss: 0.5810, val_loss: 0.1420,  acc: 0.3451, val_acc: 0.2888\n",
      "Epoch [88/200], loss: 0.5819, val_loss: 0.1418,  acc: 0.3469, val_acc: 0.2883\n",
      "Epoch [89/200], loss: 0.5760, val_loss: 0.1417,  acc: 0.3445, val_acc: 0.2883\n",
      "Epoch [90/200], loss: 0.5768, val_loss: 0.1417,  acc: 0.3458, val_acc: 0.3119\n",
      "Epoch [91/200], loss: 0.5807, val_loss: 0.1422,  acc: 0.3490, val_acc: 0.2883\n",
      "Epoch [92/200], loss: 0.5842, val_loss: 0.1422,  acc: 0.3452, val_acc: 0.3113\n",
      "Epoch [93/200], loss: 0.5809, val_loss: 0.1420,  acc: 0.3459, val_acc: 0.2906\n",
      "Epoch [94/200], loss: 0.5739, val_loss: 0.1417,  acc: 0.3481, val_acc: 0.3107\n",
      "Epoch [95/200], loss: 0.5800, val_loss: 0.1419,  acc: 0.3511, val_acc: 0.3107\n",
      "Epoch [96/200], loss: 0.5794, val_loss: 0.1420,  acc: 0.3436, val_acc: 0.2888\n",
      "Epoch [97/200], loss: 0.5801, val_loss: 0.1419,  acc: 0.3517, val_acc: 0.2883\n",
      "Epoch [98/200], loss: 0.5828, val_loss: 0.1422,  acc: 0.3485, val_acc: 0.3107\n",
      "Epoch [99/200], loss: 0.5803, val_loss: 0.1420,  acc: 0.3416, val_acc: 0.2888\n",
      "Epoch [100/200], loss: 0.5824, val_loss: 0.1421,  acc: 0.3457, val_acc: 0.2888\n",
      "Epoch [101/200], loss: 0.5811, val_loss: 0.1417,  acc: 0.3492, val_acc: 0.2888\n",
      "Epoch [102/200], loss: 0.5801, val_loss: 0.1417,  acc: 0.3481, val_acc: 0.2911\n",
      "Epoch [103/200], loss: 0.5775, val_loss: 0.1418,  acc: 0.3515, val_acc: 0.2888\n",
      "Epoch [104/200], loss: 0.5805, val_loss: 0.1419,  acc: 0.3461, val_acc: 0.2888\n",
      "Epoch [105/200], loss: 0.5846, val_loss: 0.1421,  acc: 0.3441, val_acc: 0.2911\n",
      "Epoch [106/200], loss: 0.5807, val_loss: 0.1420,  acc: 0.3415, val_acc: 0.2888\n",
      "Epoch [107/200], loss: 0.5749, val_loss: 0.1418,  acc: 0.3441, val_acc: 0.2911\n",
      "Epoch [108/200], loss: 0.5817, val_loss: 0.1418,  acc: 0.3482, val_acc: 0.2883\n",
      "Epoch [109/200], loss: 0.5818, val_loss: 0.1420,  acc: 0.3480, val_acc: 0.2911\n",
      "Epoch [110/200], loss: 0.5770, val_loss: 0.1419,  acc: 0.3452, val_acc: 0.3107\n",
      "Epoch [111/200], loss: 0.5776, val_loss: 0.1417,  acc: 0.3468, val_acc: 0.2888\n",
      "Epoch [112/200], loss: 0.5794, val_loss: 0.1419,  acc: 0.3426, val_acc: 0.2888\n",
      "Epoch [113/200], loss: 0.5793, val_loss: 0.1418,  acc: 0.3398, val_acc: 0.2911\n",
      "Epoch [114/200], loss: 0.5812, val_loss: 0.1418,  acc: 0.3478, val_acc: 0.2888\n",
      "Epoch [115/200], loss: 0.5824, val_loss: 0.1422,  acc: 0.3435, val_acc: 0.2883\n",
      "Epoch [116/200], loss: 0.5770, val_loss: 0.1418,  acc: 0.3465, val_acc: 0.3107\n",
      "Epoch [117/200], loss: 0.5791, val_loss: 0.1417,  acc: 0.3444, val_acc: 0.2911\n",
      "Epoch [118/200], loss: 0.5808, val_loss: 0.1419,  acc: 0.3551, val_acc: 0.2911\n",
      "Epoch [119/200], loss: 0.5765, val_loss: 0.1418,  acc: 0.3482, val_acc: 0.2911\n",
      "Epoch [120/200], loss: 0.5803, val_loss: 0.1418,  acc: 0.3451, val_acc: 0.2911\n",
      "Epoch [121/200], loss: 0.5727, val_loss: 0.1416,  acc: 0.3531, val_acc: 0.2911\n",
      "Epoch [122/200], loss: 0.5783, val_loss: 0.1419,  acc: 0.3467, val_acc: 0.2911\n",
      "Epoch [123/200], loss: 0.5798, val_loss: 0.1421,  acc: 0.3446, val_acc: 0.3199\n",
      "Epoch [124/200], loss: 0.5811, val_loss: 0.1420,  acc: 0.3491, val_acc: 0.2888\n",
      "Epoch [125/200], loss: 0.5791, val_loss: 0.1420,  acc: 0.3455, val_acc: 0.2923\n",
      "Epoch [126/200], loss: 0.5773, val_loss: 0.1420,  acc: 0.3444, val_acc: 0.2911\n",
      "Epoch [127/200], loss: 0.5789, val_loss: 0.1422,  acc: 0.3513, val_acc: 0.2739\n",
      "Epoch [128/200], loss: 0.5793, val_loss: 0.1419,  acc: 0.3438, val_acc: 0.2911\n",
      "Epoch [129/200], loss: 0.5781, val_loss: 0.1421,  acc: 0.3444, val_acc: 0.3119\n",
      "Epoch [130/200], loss: 0.5760, val_loss: 0.1417,  acc: 0.3540, val_acc: 0.2911\n",
      "Epoch [131/200], loss: 0.5803, val_loss: 0.1419,  acc: 0.3446, val_acc: 0.2911\n",
      "Epoch [132/200], loss: 0.5807, val_loss: 0.1418,  acc: 0.3500, val_acc: 0.2911\n",
      "Epoch [133/200], loss: 0.5805, val_loss: 0.1419,  acc: 0.3449, val_acc: 0.2911\n",
      "Epoch [134/200], loss: 0.5761, val_loss: 0.1418,  acc: 0.3491, val_acc: 0.2911\n",
      "Epoch [135/200], loss: 0.5789, val_loss: 0.1417,  acc: 0.3504, val_acc: 0.2911\n",
      "Epoch [136/200], loss: 0.5755, val_loss: 0.1417,  acc: 0.3485, val_acc: 0.2888\n",
      "Epoch [137/200], loss: 0.5747, val_loss: 0.1419,  acc: 0.3508, val_acc: 0.2911\n",
      "Epoch [138/200], loss: 0.5795, val_loss: 0.1416,  acc: 0.3491, val_acc: 0.2911\n",
      "Epoch [139/200], loss: 0.5789, val_loss: 0.1419,  acc: 0.3464, val_acc: 0.2911\n",
      "Epoch [140/200], loss: 0.5749, val_loss: 0.1417,  acc: 0.3492, val_acc: 0.2911\n",
      "Epoch [141/200], loss: 0.5772, val_loss: 0.1418,  acc: 0.3471, val_acc: 0.2911\n",
      "Epoch [142/200], loss: 0.5794, val_loss: 0.1419,  acc: 0.3484, val_acc: 0.2911\n",
      "Epoch [143/200], loss: 0.5762, val_loss: 0.1418,  acc: 0.3500, val_acc: 0.2992\n",
      "Epoch [144/200], loss: 0.5782, val_loss: 0.1416,  acc: 0.3549, val_acc: 0.2911\n",
      "Epoch [145/200], loss: 0.5774, val_loss: 0.1417,  acc: 0.3505, val_acc: 0.3119\n",
      "Epoch [146/200], loss: 0.5778, val_loss: 0.1418,  acc: 0.3471, val_acc: 0.3119\n",
      "Epoch [147/200], loss: 0.5831, val_loss: 0.1421,  acc: 0.3478, val_acc: 0.2923\n",
      "Epoch [148/200], loss: 0.5709, val_loss: 0.1416,  acc: 0.3515, val_acc: 0.2923\n",
      "Epoch [149/200], loss: 0.5787, val_loss: 0.1418,  acc: 0.3438, val_acc: 0.2911\n",
      "Epoch [150/200], loss: 0.5763, val_loss: 0.1418,  acc: 0.3561, val_acc: 0.2911\n",
      "Epoch [151/200], loss: 0.5759, val_loss: 0.1417,  acc: 0.3553, val_acc: 0.2911\n",
      "Epoch [152/200], loss: 0.5812, val_loss: 0.1419,  acc: 0.3504, val_acc: 0.3257\n",
      "Epoch [153/200], loss: 0.5739, val_loss: 0.1417,  acc: 0.3508, val_acc: 0.2923\n",
      "Epoch [154/200], loss: 0.5788, val_loss: 0.1418,  acc: 0.3515, val_acc: 0.2911\n",
      "Epoch [155/200], loss: 0.5817, val_loss: 0.1417,  acc: 0.3538, val_acc: 0.2911\n",
      "Epoch [156/200], loss: 0.5801, val_loss: 0.1418,  acc: 0.3511, val_acc: 0.2911\n",
      "Epoch [157/200], loss: 0.5755, val_loss: 0.1416,  acc: 0.3510, val_acc: 0.3119\n",
      "Epoch [158/200], loss: 0.5751, val_loss: 0.1416,  acc: 0.3543, val_acc: 0.2911\n",
      "Epoch [159/200], loss: 0.5830, val_loss: 0.1420,  acc: 0.3510, val_acc: 0.2911\n",
      "Epoch [160/200], loss: 0.5705, val_loss: 0.1416,  acc: 0.3497, val_acc: 0.2911\n",
      "Epoch [161/200], loss: 0.5775, val_loss: 0.1419,  acc: 0.3458, val_acc: 0.3119\n",
      "Epoch [162/200], loss: 0.5785, val_loss: 0.1418,  acc: 0.3484, val_acc: 0.2911\n",
      "Epoch [163/200], loss: 0.5746, val_loss: 0.1417,  acc: 0.3501, val_acc: 0.2911\n",
      "Epoch [164/200], loss: 0.5767, val_loss: 0.1420,  acc: 0.3524, val_acc: 0.3176\n",
      "Epoch [165/200], loss: 0.5768, val_loss: 0.1418,  acc: 0.3605, val_acc: 0.3119\n",
      "Epoch [166/200], loss: 0.5780, val_loss: 0.1419,  acc: 0.3528, val_acc: 0.2911\n",
      "Epoch [167/200], loss: 0.5766, val_loss: 0.1418,  acc: 0.3478, val_acc: 0.2911\n",
      "Epoch [168/200], loss: 0.5752, val_loss: 0.1417,  acc: 0.3484, val_acc: 0.2911\n",
      "Epoch [169/200], loss: 0.5743, val_loss: 0.1418,  acc: 0.3501, val_acc: 0.3199\n",
      "Epoch [170/200], loss: 0.5762, val_loss: 0.1416,  acc: 0.3577, val_acc: 0.2911\n",
      "Epoch [171/200], loss: 0.5800, val_loss: 0.1416,  acc: 0.3507, val_acc: 0.2911\n",
      "Epoch [172/200], loss: 0.5789, val_loss: 0.1418,  acc: 0.3458, val_acc: 0.2911\n",
      "Epoch [173/200], loss: 0.5727, val_loss: 0.1415,  acc: 0.3515, val_acc: 0.2911\n",
      "Epoch [174/200], loss: 0.5774, val_loss: 0.1415,  acc: 0.3513, val_acc: 0.2911\n",
      "Epoch [175/200], loss: 0.5734, val_loss: 0.1419,  acc: 0.3469, val_acc: 0.3176\n",
      "Epoch [176/200], loss: 0.5791, val_loss: 0.1417,  acc: 0.3501, val_acc: 0.2969\n",
      "Epoch [177/200], loss: 0.5800, val_loss: 0.1419,  acc: 0.3520, val_acc: 0.2980\n",
      "Epoch [178/200], loss: 0.5807, val_loss: 0.1420,  acc: 0.3474, val_acc: 0.2911\n",
      "Epoch [179/200], loss: 0.5814, val_loss: 0.1418,  acc: 0.3434, val_acc: 0.2911\n",
      "Epoch [180/200], loss: 0.5769, val_loss: 0.1416,  acc: 0.3511, val_acc: 0.2911\n",
      "Epoch [181/200], loss: 0.5733, val_loss: 0.1418,  acc: 0.3468, val_acc: 0.2969\n",
      "Epoch [182/200], loss: 0.5770, val_loss: 0.1418,  acc: 0.3523, val_acc: 0.2911\n",
      "Epoch [183/200], loss: 0.5797, val_loss: 0.1416,  acc: 0.3504, val_acc: 0.2911\n",
      "Epoch [184/200], loss: 0.5782, val_loss: 0.1418,  acc: 0.3510, val_acc: 0.2969\n",
      "Epoch [185/200], loss: 0.5813, val_loss: 0.1419,  acc: 0.3511, val_acc: 0.2969\n",
      "Epoch [186/200], loss: 0.5737, val_loss: 0.1419,  acc: 0.3490, val_acc: 0.2969\n",
      "Epoch [187/200], loss: 0.5758, val_loss: 0.1418,  acc: 0.3520, val_acc: 0.2969\n",
      "Epoch [188/200], loss: 0.5778, val_loss: 0.1418,  acc: 0.3508, val_acc: 0.3165\n",
      "Epoch [189/200], loss: 0.5758, val_loss: 0.1417,  acc: 0.3566, val_acc: 0.2969\n",
      "Epoch [190/200], loss: 0.5800, val_loss: 0.1420,  acc: 0.3478, val_acc: 0.3165\n",
      "Epoch [191/200], loss: 0.5727, val_loss: 0.1418,  acc: 0.3481, val_acc: 0.2969\n",
      "Epoch [192/200], loss: 0.5773, val_loss: 0.1421,  acc: 0.3498, val_acc: 0.3165\n",
      "Epoch [193/200], loss: 0.5762, val_loss: 0.1417,  acc: 0.3590, val_acc: 0.2969\n",
      "Epoch [194/200], loss: 0.5807, val_loss: 0.1418,  acc: 0.3480, val_acc: 0.2969\n",
      "Epoch [195/200], loss: 0.5744, val_loss: 0.1418,  acc: 0.3546, val_acc: 0.3165\n",
      "Epoch [196/200], loss: 0.5803, val_loss: 0.1418,  acc: 0.3504, val_acc: 0.2969\n",
      "Epoch [197/200], loss: 0.5730, val_loss: 0.1416,  acc: 0.3536, val_acc: 0.2969\n",
      "Epoch [198/200], loss: 0.5797, val_loss: 0.1417,  acc: 0.3524, val_acc: 0.2969\n",
      "Epoch [199/200], loss: 0.5787, val_loss: 0.1418,  acc: 0.3501, val_acc: 0.3165\n",
      "Epoch [200/200], loss: 0.5720, val_loss: 0.1416,  acc: 0.3549, val_acc: 0.2963\n",
      "CV loss 0.13977783675546998 CV acc 0.3039235278664312\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "logs = {\n",
    "    f'cv{i}': {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'acc': [],\n",
    "        'val_acc': [],\n",
    "    }\n",
    "    for i in range(1, 6)\n",
    "}\n",
    "logs['cv_acc'] = 0\n",
    "logs['cv_loss'] = 0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for _fold, (train_index, valid_index) in enumerate(kf.split(train_set), 1):\n",
    "    print(f'---------- cv{_fold} ------------')\n",
    "    model = MLP(x_train.shape[1], 1, num_hidden, lr=lr).to(device)\n",
    "    \n",
    "    train_dataset = Subset(train_set, train_index)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    valid_dataset = Subset(train_set, valid_index)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        # train\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model.training_step((X, y))\n",
    "            train_loss += outputs['loss'].item()\n",
    "            train_acc += torch.sum((outputs['pred'] >= 0.5) * (y == 1), axis=0).item()\n",
    "        train_loss = train_loss / i  # batchごとにlossがmeanされてるため、iteration数で割ることで全batchの平均lossになる\n",
    "        train_acc = train_acc / len(train_loader.dataset)\n",
    "\n",
    "        # val\n",
    "        for X, y in valid_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model.validation_step((X, y))\n",
    "            val_loss += outputs['loss'].item()\n",
    "            val_acc += torch.sum((outputs['pred'] >= 0.5) * (y == 1), axis=0).item()\n",
    "        val_loss = val_loss / i\n",
    "        val_acc = val_acc / len(valid_loader.dataset)\n",
    "\n",
    "        print (f'Epoch [{epoch+1}/{epochs}], loss: {train_loss:.4f}, val_loss: {val_loss:.4f},  acc: {train_acc:.4f}, val_acc: {val_acc:.4f}')\n",
    "        logs[f'cv{_fold}']['loss'].append(train_loss)\n",
    "        logs[f'cv{_fold}']['acc'].append(train_acc)\n",
    "        logs[f'cv{_fold}']['val_loss'].append(val_loss)\n",
    "        logs[f'cv{_fold}']['val_acc'].append(val_acc)\n",
    "\n",
    "    logs[f'cv{_fold}']['model'] = model\n",
    "    logs['cv_loss'] += val_loss / kf.n_splits\n",
    "    logs['cv_acc'] += val_acc / kf.n_splits\n",
    "\n",
    "print('CV loss', logs['cv_loss'], 'CV acc', logs['cv_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba5b40-7c2e-47c9-9195-00fb03c1a1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
